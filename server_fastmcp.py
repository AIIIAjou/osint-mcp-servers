#!/usr/bin/env python3
"""
OSINT Unified MCP Server - fastmcp Architecture
í†µí•© OSINT MCP ì„œë²„ - fastmcp ê¸°ë°˜

Phase 1 êµ¬í˜„: ê¸°ì¡´ server.pyì˜ 7ê°œ toolì„ fastmcpë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
ëª©í‘œ: ë‹¨ì¼ /mcp ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ëª¨ë“  OSINT ë„êµ¬ ì œê³µ
"""

import os
import json
import time
import logging
import asyncio
import subprocess
import base64
from typing import Any, Dict, List, Optional
from datetime import datetime, timezone

from dotenv import load_dotenv

try:
    from intelxapi import intelx
except ImportError:
    # Fallback if intelxapi not available
    intelx = None
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from fastapi import FastAPI, HTTPException, Request, Query
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

import requests

# ============================================================================
# Phase 0: ì´ˆê¸°í™” ë° í™˜ê²½ì„¤ì •
# ============================================================================

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# API ì„¤ì •
INTELX_API_KEY = os.getenv("INTELX_API_KEY", "")
VIRUSTOTAL_API_KEY = os.getenv("VIRUSTOTAL_API_KEY", "")
SHODAN_API_KEY = os.getenv("SHODAN_API_KEY", "")
HARVESTER_API_KEY = os.getenv("HARVESTER_API_KEY", "")
DEBUG_MODE = os.getenv("DEBUG_MODE", "false").lower() == "true"

# API í‚¤ ìœ íš¨ì„± í™•ì¸
if not INTELX_API_KEY and not DEBUG_MODE:
    logger.warning("Intelligence X API KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

if not VIRUSTOTAL_API_KEY and not DEBUG_MODE:
    logger.warning(
        "VirusTotal API KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (DEBUG_MODE=trueì¸ ê²½ìš° Mock ë°ì´í„° ì‚¬ìš©)"
    )

if DEBUG_MODE:
    logger.info("ğŸ”§ DEBUG_MODE í™œì„±í™” - Mock ë°ì´í„° ì‚¬ìš©")

# ============================================================================
# Phase 1: Pydantic ëª¨ë¸
# ============================================================================


class SearchRequest(BaseModel):
    term: str = Field(..., description="ê²€ìƒ‰í•  ì…€ë ‰í„° (ì´ë©”ì¼, ë„ë©”ì¸, IP ë“±)")
    maxresults: int = Field(100, description="ìµœëŒ€ ê²°ê³¼ ìˆ˜")
    timeout: int = Field(5, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)")
    buckets: Optional[List[str]] = Field(None, description="ê²€ìƒ‰í•  ë²„í‚· ëª©ë¡")
    datefrom: Optional[str] = Field(None, description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    dateto: Optional[str] = Field(None, description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")


class SherlockSearchRequest(BaseModel):
    username: str = Field(..., description="ê²€ìƒ‰í•  ì‚¬ìš©ìëª…")
    sites: Optional[List[str]] = Field(
        None, description="ê²€ìƒ‰í•  ì‚¬ì´íŠ¸ ëª©ë¡ (ì˜ˆ: ['github', 'twitter'])"
    )
    timeout: int = Field(120, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 120ì´ˆ)")


class PlaywrightAnalyzeRequest(BaseModel):
    url: str = Field(..., description="ë¶„ì„í•  URL")
    extract_metadata: bool = Field(
        True, description="ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì œëª©, ì„¤ëª…, ì´ë¯¸ì§€)"
    )
    extract_text: bool = Field(True, description="í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    extract_links: bool = Field(True, description="ë§í¬ ëª©ë¡ ì¶”ì¶œ")
    screenshot: bool = Field(False, description="ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜")
    wait_for_selector: Optional[str] = Field(
        None, description="íŠ¹ì • ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°"
    )
    timeout: int = Field(30, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


class PlaywrightCrawlRequest(BaseModel):
    url: str = Field(..., description="ì‹œì‘í•  URL")
    max_depth: int = Field(2, description="í¬ë¡¤ë§ ê¹Šì´ (ê¸°ë³¸ê°’: 2)")
    max_pages: int = Field(10, description="ìµœëŒ€ ë°©ë¬¸ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 10)")
    url_pattern: Optional[str] = Field(
        None, description="í¬ë¡¤ë§í•  URL íŒ¨í„´ (ì •ê·œí‘œí˜„ì‹, ì˜ˆ: .*github.com.*)"
    )
    extract_text: bool = Field(True, description="í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    extract_links: bool = Field(True, description="ë§í¬ ì¶”ì¶œ")
    analyze_content: bool = Field(True, description="ì§€ëŠ¥í˜• ì½˜í…ì¸  ë¶„ì„")
    timeout: int = Field(60, description="ì „ì²´ í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


class ThreatIntelRequest(BaseModel):
    query: str = Field(..., description="ì¡°íšŒí•  ëŒ€ìƒ (ë„ë©”ì¸ ë˜ëŠ” IP)")
    query_type: str = Field("domain", description="ì¡°íšŒ íƒ€ì…: domain ë˜ëŠ” ip")
    timeout: int = Field(10, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


# ============================================================================
# Phase 2: Client Classes (ê¸°ì¡´ êµ¬í˜„ ìœ ì§€)
# ============================================================================


class IntelligenceXClient:
    """Intelligence X ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, api_key: str):
        self.client = None
        self.debug_mode = DEBUG_MODE

        if api_key and intelx is not None:
            self.client = intelx(api_key)
            self.client.API_ROOT = "https://free.intelx.io"

    def search(self, search_request: SearchRequest) -> Dict[str, Any]:
        if self.debug_mode:
            logger.info(f"DEBUG MODE: Mock ë°ì´í„° ë°˜í™˜ (ê²€ìƒ‰ì–´: {search_request.term})")
            return {
                "records": [
                    {
                        "name": f"Mock Result 1 for {search_request.term}",
                        "description": "This is a mock result for testing purposes",
                        "date": datetime.now().isoformat(),
                        "media": 1,
                        "type": 1,
                        "added": datetime.now().isoformat(),
                        "storageid": "mock-storage-id-1",
                        "bucket": "mock-bucket",
                    },
                    {
                        "name": f"Mock Result 2 for {search_request.term}",
                        "description": "Second mock result for testing",
                        "date": "2025-10-25T12:00:00",
                        "media": 1,
                        "type": 1,
                        "added": "2025-10-25T14:30:00",
                        "storageid": "mock-storage-id-2",
                        "bucket": "mock-bucket",
                    },
                ]
            }

        try:
            if not self.client:
                raise HTTPException(
                    status_code=500, detail="API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                )

            results = self.client.search(
                search_request.term,
                maxresults=search_request.maxresults,
                timeout=search_request.timeout,
            )
            return results
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise HTTPException(
                status_code=500, detail=f"Intelligence X API ì˜¤ë¥˜: {str(e)}"
            )


class SherlockClient:
    """Sherlock ì‚¬ìš©ìëª… ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.debug_mode = DEBUG_MODE

    def search(self, search_request: SherlockSearchRequest) -> Dict[str, Any]:
        """Sherlockìœ¼ë¡œ ì‚¬ìš©ìëª… ê²€ìƒ‰"""
        if self.debug_mode:
            logger.info(
                f"DEBUG MODE: Sherlock Mock ë°ì´í„° ë°˜í™˜ (ì‚¬ìš©ìëª…: {search_request.username})"
            )
            return {
                "found": {
                    "github": {
                        "url": f"https://github.com/{search_request.username}",
                        "status": "found",
                    },
                    "twitter": {
                        "url": f"https://twitter.com/{search_request.username}",
                        "status": "found",
                    },
                },
                "not_found": ["instagram", "reddit"],
                "total_found": 2,
                "total_checked": 4,
            }

        try:
            cmd = ["sherlock", search_request.username, "--no-color", "--no-txt"]

            if search_request.sites:
                for site in search_request.sites:
                    cmd.extend(["--site", site])

            logger.info(f"Sherlock ê²€ìƒ‰ ì‹¤í–‰: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=search_request.timeout * len(search_request.sites or [100]),
            )

            found_accounts = {}
            lines = result.stdout.split("\n")

            for line in lines:
                if line.strip().startswith("[+]"):
                    parts = line.strip()[4:].split(": ", 1)
                    if len(parts) == 2:
                        site_name = parts[0].strip()
                        url = parts[1].strip()
                        found_accounts[site_name] = {"url": url, "status": "found"}

            total_found = len(found_accounts)

            return {
                "found": found_accounts,
                "total_found": total_found,
                "username": search_request.username,
                "timestamp": datetime.now().isoformat(),
                "status": "completed",
            }
        except subprocess.TimeoutExpired:
            logger.error(f"Sherlock íƒ€ì„ì•„ì›ƒ: {search_request.username}")
            raise HTTPException(status_code=408, detail="Sherlock ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            logger.error(f"Sherlock ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"Sherlock ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")


class PlaywrightClient:
    """Playwright URL ë¶„ì„ ë° ìë™ í¬ë¡¤ë§ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.debug_mode = DEBUG_MODE
        self.visited_urls = set()
        self.crawl_results = []

    async def analyze(
        self, analyze_request: PlaywrightAnalyzeRequest
    ) -> Dict[str, Any]:
        """Playwrightë¡œ URL ë¶„ì„"""
        if self.debug_mode:
            logger.info(
                f"DEBUG MODE: Playwright Mock ë°ì´í„° ë°˜í™˜ (URL: {analyze_request.url})"
            )
            return {
                "url": analyze_request.url,
                "metadata": {
                    "title": "Mock Page Title",
                    "description": "This is a mock page description",
                    "image": "https://example.com/image.jpg",
                },
                "text": "Mock page content...",
                "links": [
                    {"text": "Link 1", "href": "https://example.com/link1"},
                    {"text": "Link 2", "href": "https://example.com/link2"},
                ],
                "screenshot": None,
                "status": "completed",
            }

        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch()
                page = await browser.new_page()

                logger.info(f"Playwright í˜ì´ì§€ ë¡œë“œ: {analyze_request.url}")
                await page.goto(
                    analyze_request.url,
                    timeout=analyze_request.timeout * 1000,
                    wait_until="load",
                )

                if analyze_request.wait_for_selector:
                    await page.wait_for_selector(
                        analyze_request.wait_for_selector, timeout=5000
                    )

                result = {"url": analyze_request.url, "status": "completed"}

                if analyze_request.extract_metadata:
                    title = await page.title()

                    try:
                        meta_description = await page.locator(
                            'meta[name="description"]'
                        ).get_attribute("content", timeout=1000)
                    except:
                        meta_description = None

                    try:
                        meta_image = await page.locator(
                            'meta[property="og:image"]'
                        ).get_attribute("content", timeout=1000)
                    except:
                        meta_image = None

                    result["metadata"] = {
                        "title": title,
                        "description": meta_description or "",
                        "image": meta_image or "",
                    }

                if analyze_request.extract_text:
                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")
                    for script in soup(["script", "style"]):
                        script.decompose()
                    text = soup.get_text(separator="\n", strip=True)
                    result["text"] = text[:2000] if text else ""

                if analyze_request.extract_links:
                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")
                    links = []
                    for link in soup.find_all("a", href=True):
                        links.append(
                            {"text": link.get_text(strip=True), "href": link["href"]}
                        )
                    result["links"] = links[:50]

                if analyze_request.screenshot:
                    screenshot_bytes = await page.screenshot()
                    result["screenshot"] = base64.b64encode(screenshot_bytes).decode(
                        "utf-8"
                    )

                await browser.close()
                return result

        except Exception as e:
            logger.error(f"Playwright ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"URL ë¶„ì„ ì˜¤ë¥˜: {str(e)}")

    def _analyze_content(self, html: str, text: str, url: str) -> Dict[str, Any]:
        """í˜ì´ì§€ ì½˜í…ì¸ ì˜ ì§€ëŠ¥í˜• ë¶„ì„"""
        import re

        analysis = {
            "page_purpose": self._detect_page_purpose(text, html, url),
            "key_information": self._extract_key_info(text, html),
            "potential_risks": self._detect_risks(text, html, url),
            "entities": self._extract_entities(text),
            "keywords": self._extract_keywords(text),
        }
        return analysis

    def _detect_page_purpose(self, text: str, html: str, url: str) -> str:
        """í˜ì´ì§€ì˜ ëª©ì  íŒŒì•…"""
        text_lower = text.lower()
        url_lower = url.lower()

        # í”„ë¡œí•„ í˜ì´ì§€ ê°ì§€
        if any(keyword in text_lower for keyword in ["profile", "about", "bio", "user"]) or \
           any(keyword in url_lower for keyword in ["profile", "user", "about"]):
            return "User/Profile Page"

        # ë¡œê·¸ì¸ í˜ì´ì§€
        if any(keyword in text_lower for keyword in ["login", "password", "username", "sign in"]):
            return "Authentication/Login Page"

        # ìƒê±°ë˜ ì‚¬ì´íŠ¸
        if any(keyword in text_lower for keyword in ["price", "buy", "purchase", "cart", "checkout", "product"]):
            return "E-commerce/Shopping Page"

        # ë¬¸ì„œ/ë¸”ë¡œê·¸
        if any(keyword in text_lower for keyword in ["article", "blog", "post", "author", "published"]):
            return "Blog/Article Page"

        # ê²€ìƒ‰ ê²°ê³¼
        if any(keyword in text_lower for keyword in ["search result", "found", "matches"]):
            return "Search Results Page"

        # API í˜ì´ì§€
        if "api" in url_lower or any(keyword in text_lower for keyword in ["endpoint", "request", "response", "json"]):
            return "API/Technical Documentation"

        return "General Content Page"

    def _extract_key_info(self, text: str, html: str) -> Dict[str, Any]:
        """ì£¼ìš” ì •ë³´ ì¶”ì¶œ"""
        import re

        info = {}

        # ì´ë©”ì¼ ì¶”ì¶œ
        emails = set(re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text))
        if emails:
            info["emails"] = list(emails)[:5]  # ìµœëŒ€ 5ê°œ

        # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
        phones = set(re.findall(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', text))
        if phones:
            info["phone_numbers"] = list(phones)[:5]

        # ì†Œì…œ ë¯¸ë””ì–´ ë§í¬ ì¶”ì¶œ
        social_patterns = {
            "twitter": r'twitter\.com/[\w]+',
            "github": r'github\.com/[\w-]+',
            "linkedin": r'linkedin\.com/in/[\w-]+',
            "instagram": r'instagram\.com/[\w.]+',
            "facebook": r'facebook\.com/[\w.]+',
        }

        social_links = {}
        for platform, pattern in social_patterns.items():
            matches = re.findall(pattern, text)
            if matches:
                social_links[platform] = list(set(matches))[:3]

        if social_links:
            info["social_media"] = social_links

        # HTML ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´
        soup = BeautifulSoup(html, "html.parser")

        # Open Graph ì •ë³´
        og_title = soup.find("meta", property="og:title")
        og_description = soup.find("meta", property="og:description")

        if og_title:
            info["og_title"] = og_title.get("content")
        if og_description:
            info["og_description"] = og_description.get("content")

        # ì¡°ì§ëª… ì¶”ì¶œ ì‹œë„
        author = soup.find("meta", {"name": "author"})
        if author:
            info["author"] = author.get("content")

        return info

    def _detect_risks(self, text: str, html: str, url: str) -> List[str]:
        """ì ì¬ì  ë³´ì•ˆ ìœ„í—˜ ê°ì§€"""
        risks = []
        text_lower = text.lower()
        url_lower = url.lower()

        # í”¼ì‹± ì§•í›„
        if any(keyword in text_lower for keyword in ["verify account", "confirm identity", "update payment", "urgent action required"]):
            risks.append("Potential phishing indicators")

        # ì¸ì¦ ìš”êµ¬
        if "password" in text_lower or "login" in text_lower:
            risks.append("Authentication required - verify legitimacy")

        # ì˜ì‹¬ í™œë™
        if any(keyword in text_lower for keyword in ["limited time", "act now", "verify immediately", "confirm now"]):
            risks.append("High-pressure/urgency language detected")

        # ì™¸ë¶€ ìŠ¤í¬ë¦½íŠ¸ (XSS ê°€ëŠ¥ì„±)
        if "<script" in html and "src=" in html:
            script_count = html.count("<script")
            if script_count > 5:
                risks.append(f"Multiple external scripts detected ({script_count})")

        # HTTP vs HTTPS
        if url_lower.startswith("http://") and any(keyword in text_lower for keyword in ["password", "payment", "card"]):
            risks.append("Sensitive data handling over insecure HTTP")

        # ìˆ¨ê²¨ì§„ í¼
        if "<form" in html and "style" in html.lower():
            if "display:none" in html.lower() or "visibility:hidden" in html.lower():
                risks.append("Hidden form elements detected")

        return risks if risks else ["No obvious security risks detected"]

    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜)"""
        import re

        entities = {}

        # URL ì¶”ì¶œ
        urls = re.findall(r'https?://[^\s<>"{}|\\^`\[\]]*', text)
        if urls:
            entities["urls"] = list(set(urls))[:10]

        # IP ì£¼ì†Œ ì¶”ì¶œ
        ips = re.findall(r'\b(?:\d{1,3}\.){3}\d{1,3}\b', text)
        if ips:
            entities["ip_addresses"] = list(set(ips))[:10]

        # ë„ë©”ì¸ ì¶”ì¶œ
        domains = re.findall(r'\b(?:[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?\.)+[a-z]{2,}\b', text.lower())
        if domains:
            entities["domains"] = list(set(domains))[:10]

        return entities

    def _extract_keywords(self, text: str) -> List[str]:
        """ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ìˆœ ë¹ˆë„ ê¸°ë°˜)"""
        # ë¶ˆìš©ì–´ ì œì™¸
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                     'of', 'is', 'was', 'are', 'be', 'have', 'has', 'do', 'does', 'did',
                     'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'}

        words = text.lower().split()
        # ê¸¸ì´ 4 ì´ìƒì˜ ë‹¨ì–´ë§Œ ê³ ë ¤
        filtered_words = [w for w in words if len(w) > 4 and w not in stopwords and w.isalpha()]

        # ë¹ˆë„ ê³„ì‚°
        from collections import Counter
        word_freq = Counter(filtered_words)

        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
        keywords = [word for word, _ in word_freq.most_common(10)]
        return keywords

    async def crawl(
        self, crawl_request: PlaywrightCrawlRequest
    ) -> Dict[str, Any]:
        """ìë™ í¬ë¡¤ë§ ë° ì§€ëŠ¥í˜• ë¶„ì„"""
        import re
        from urllib.parse import urljoin, urlparse

        if self.debug_mode:
            logger.info(f"DEBUG MODE: Crawler Mock ë°ì´í„° ë°˜í™˜ (URL: {crawl_request.url})")
            return {
                "start_url": crawl_request.url,
                "pages_crawled": 3,
                "max_depth": crawl_request.max_depth,
                "summary": {
                    "primary_purpose": "Mock User Profile Page",
                    "key_findings": ["4 repositories", "Public profile", "Active developer"],
                    "social_links": {"github": ["https://github.com/kjmkjmkj"]},
                    "risks": ["No obvious security risks detected"],
                },
                "pages": [
                    {
                        "url": crawl_request.url,
                        "title": "Mock Profile",
                        "purpose": "User/Profile Page",
                        "key_info": {"social_media": {"github": ["https://github.com/kjmkjmkj"]}},
                        "depth": 0,
                    }
                ],
                "status": "completed",
                "note": "Mock data in DEBUG_MODE",
            }

        try:
            self.visited_urls = set()
            self.crawl_results = []

            start_time = time.time()
            async with async_playwright() as p:
                browser = await p.chromium.launch()

                await self._crawl_recursive(
                    crawl_request.url,
                    browser,
                    crawl_request,
                    depth=0
                )

                await browser.close()

            execution_time = time.time() - start_time

            # ë¶„ì„ ê²°ê³¼ ì¢…í•©
            summary = self._generate_summary(self.crawl_results, crawl_request.url)

            return {
                "start_url": crawl_request.url,
                "pages_crawled": len(self.crawl_results),
                "max_depth": crawl_request.max_depth,
                "summary": summary,
                "pages": self.crawl_results,
                "status": "completed",
                "execution_time_ms": int(execution_time * 1000),
            }

        except Exception as e:
            logger.error(f"Playwright í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")

    async def _crawl_recursive(
        self,
        url: str,
        browser,
        crawl_request: PlaywrightCrawlRequest,
        depth: int
    ) -> None:
        """ì¬ê·€ì  í¬ë¡¤ë§"""
        import re
        from urllib.parse import urljoin, urlparse

        # ë°©ë¬¸ ì œí•œ í™•ì¸
        if url in self.visited_urls:
            return
        if len(self.crawl_results) >= crawl_request.max_pages:
            return
        if depth > crawl_request.max_depth:
            return

        # URL íŒ¨í„´ í™•ì¸
        if crawl_request.url_pattern:
            if not re.search(crawl_request.url_pattern, url):
                return

        # ê°™ì€ ë„ë©”ì¸ í™•ì¸
        start_domain = urlparse(crawl_request.url).netloc
        current_domain = urlparse(url).netloc
        if start_domain != current_domain:
            return

        self.visited_urls.add(url)

        try:
            page = await browser.new_page()
            logger.info(f"í¬ë¡¤ë§: {url} (ê¹Šì´: {depth})")

            await page.goto(
                url,
                timeout=crawl_request.timeout * 1000,
                wait_until="load"
            )

            # í˜ì´ì§€ ë¶„ì„
            html = await page.content()
            soup = BeautifulSoup(html, "html.parser")

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text(separator="\n", strip=True)

            # ë©”íƒ€ë°ì´í„°
            title = await page.title()

            # ë¶„ì„
            page_analysis = {}
            if crawl_request.analyze_content:
                page_analysis = self._analyze_content(html, text, url)

            # ë§í¬ ì¶”ì¶œ
            links = []
            if crawl_request.extract_links:
                for link in soup.find_all("a", href=True):
                    href = link["href"]
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    absolute_url = urljoin(url, href)
                    # í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±°
                    absolute_url = absolute_url.split("#")[0]
                    if absolute_url not in self.visited_urls:
                        links.append(absolute_url)

            # ê²°ê³¼ ì €ì¥
            self.crawl_results.append({
                "url": url,
                "depth": depth,
                "title": title,
                "text": text[:1500] if crawl_request.extract_text else "",
                **page_analysis
            })

            # ë‹¤ìŒ ë ˆë²¨ í¬ë¡¤ë§
            if depth < crawl_request.max_depth:
                for link in links[:5]:  # ê° í˜ì´ì§€ë‹¹ ìµœëŒ€ 5ê°œ ë§í¬ë§Œ ë”°ë¼ê°€ê¸°
                    if len(self.crawl_results) < crawl_request.max_pages:
                        await self._crawl_recursive(
                            link,
                            browser,
                            crawl_request,
                            depth + 1
                        )

            await page.close()

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ({url}): {e}")

    def _generate_summary(self, results: List[Dict], start_url: str) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ê²°ê³¼ ì¢…í•© ë¶„ì„"""
        if not results:
            return {"status": "no_results"}

        # ì²« ë²ˆì§¸ í˜ì´ì§€ë¥¼ ì£¼ìš” ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ
        primary = results[0]

        summary = {
            "primary_purpose": primary.get("page_purpose", "Unknown"),
            "total_pages_analyzed": len(results),
            "key_findings": [],
            "all_entities": {
                "emails": set(),
                "social_media": {},
                "urls": set(),
                "domains": set(),
            },
            "risks": set(),
            "top_keywords": [],
        }

        # ëª¨ë“  í˜ì´ì§€ì—ì„œ ì •ë³´ í†µí•©
        for result in results:
            # ë¦¬ìŠ¤í¬ í†µí•©
            if "potential_risks" in result:
                summary["risks"].update(result["potential_risks"])

            # ì—”í‹°í‹° í†µí•©
            if "entities" in result:
                for entity_type, values in result["entities"].items():
                    if entity_type in summary["all_entities"]:
                        if isinstance(values, list):
                            summary["all_entities"][entity_type].update(values)

            # ì£¼ìš” ì •ë³´
            if "key_information" in result:
                if "emails" in result["key_information"]:
                    summary["all_entities"]["emails"].update(result["key_information"]["emails"])
                if "social_media" in result["key_information"]:
                    summary["all_entities"]["social_media"].update(result["key_information"]["social_media"])

            # í‚¤ì›Œë“œ
            if "keywords" in result:
                summary["top_keywords"].extend(result["keywords"][:3])

        # Setì„ Listë¡œ ë³€í™˜
        summary["all_entities"]["emails"] = list(summary["all_entities"]["emails"])[:10]
        summary["all_entities"]["urls"] = list(summary["all_entities"]["urls"])[:10]
        summary["all_entities"]["domains"] = list(summary["all_entities"]["domains"])[:10]
        summary["risks"] = list(summary["risks"])

        # í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 10ê°œë§Œ
        from collections import Counter
        keyword_counts = Counter(summary["top_keywords"])
        summary["top_keywords"] = [word for word, _ in keyword_counts.most_common(10)]

        # ì£¼ìš” ë°œê²¬ì‚¬í•­
        if summary["all_entities"]["social_media"]:
            summary["key_findings"].append(f"Found social media links: {list(summary['all_entities']['social_media'].keys())}")
        if summary["all_entities"]["emails"]:
            summary["key_findings"].append(f"Found {len(summary['all_entities']['emails'])} email addresses")
        if len(results) > 1:
            summary["key_findings"].append(f"Crawled {len(results)} related pages")
        if summary["all_entities"]["urls"]:
            summary["key_findings"].append(f"Found {len(summary['all_entities']['urls'])} external URLs")

        return summary


class VTClient:
    """VirusTotal ìœ„í˜‘ ì •ë³´ ì¡°íšŒ í´ë˜ìŠ¤"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.debug_mode = DEBUG_MODE

    def query_domain(self, domain: str) -> Dict[str, Any]:
        """ë„ë©”ì¸ í‰íŒ ì¡°íšŒ"""
        if self.debug_mode:
            return self._mock_domain_response(domain)

        if not self.api_key:
            return {
                "status": "error",
                "error": {"code": -32001, "message": "VirusTotal API KEY ì—†ìŒ"},
            }

        try:
            headers = {"x-apikey": self.api_key}
            url = f"https://www.virustotal.com/api/v3/domains/{domain}"

            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 404:
                return {
                    "status": "error",
                    "error": {
                        "code": -32003,
                        "message": f"ë„ë©”ì¸ '{domain}'ì„(ë¥¼) VirusTotal ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    },
                }
            elif response.status_code == 429:
                return {
                    "status": "error",
                    "error": {
                        "code": -32002,
                        "message": "VirusTotal ìš”ì²­ ì œí•œ ì´ˆê³¼ - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”",
                        "data": {"retry_after": 45},
                    },
                }
            elif response.status_code != 200:
                return {
                    "status": "error",
                    "error": {
                        "code": -32000,
                        "message": f"VirusTotal ì˜¤ë¥˜: {response.status_code}",
                    },
                }

            data = response.json()
            attributes = data.get("data", {}).get("attributes", {})

            last_analysis_stats = attributes.get(
                "last_analysis_stats",
                {"malicious": 0, "suspicious": 0, "undetected": 0},
            )

            threat_level = self._calculate_threat_level(last_analysis_stats)

            return {
                "status": "success",
                "data": {
                    "domain": domain,
                    "threat_level": threat_level,
                    "detected_by": last_analysis_stats.get("malicious", 0),
                    "analysis_stats": last_analysis_stats,
                    "last_analysis_date": attributes.get("last_analysis_date"),
                    "categories": attributes.get("categories", {}),
                },
                "metadata": {
                    "source": "virustotal",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "query_time_ms": 0,
                },
            }
        except requests.exceptions.Timeout:
            return {
                "status": "error",
                "error": {"code": -32005, "message": "VirusTotal ìš”ì²­ íƒ€ì„ì•„ì›ƒ"},
            }
        except Exception as e:
            return {
                "status": "error",
                "error": {"code": -32000, "message": f"VirusTotal ì˜¤ë¥˜: {str(e)}"},
            }

    def query_ip(self, ip_address: str) -> Dict[str, Any]:
        """IP ì£¼ì†Œ í‰íŒ ì¡°íšŒ"""
        if self.debug_mode:
            return self._mock_ip_response(ip_address)

        if not self.api_key:
            return {
                "status": "error",
                "error": {"code": -32001, "message": "VirusTotal API KEY ì—†ìŒ"},
            }

        try:
            headers = {"x-apikey": self.api_key}
            url = f"https://www.virustotal.com/api/v3/ip_addresses/{ip_address}"

            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 404:
                return {
                    "status": "error",
                    "error": {
                        "code": -32003,
                        "message": f"IP '{ip_address}'ì„(ë¥¼) VirusTotal ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    },
                }
            elif response.status_code == 429:
                return {
                    "status": "error",
                    "error": {
                        "code": -32002,
                        "message": "VirusTotal ìš”ì²­ ì œí•œ ì´ˆê³¼ - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”",
                        "data": {"retry_after": 45},
                    },
                }
            elif response.status_code != 200:
                return {
                    "status": "error",
                    "error": {
                        "code": -32000,
                        "message": f"VirusTotal ì˜¤ë¥˜: {response.status_code}",
                    },
                }

            data = response.json()
            attributes = data.get("data", {}).get("attributes", {})

            last_analysis_stats = attributes.get(
                "last_analysis_stats",
                {"malicious": 0, "suspicious": 0, "undetected": 0},
            )

            threat_level = self._calculate_threat_level(last_analysis_stats)

            return {
                "status": "success",
                "data": {
                    "ip_address": ip_address,
                    "country": attributes.get("country", "Unknown"),
                    "asn": attributes.get("asn"),
                    "organization": attributes.get("as_owner", "Unknown"),
                    "threat_level": threat_level,
                    "detected_by": last_analysis_stats.get("malicious", 0),
                    "analysis_stats": last_analysis_stats,
                },
                "metadata": {
                    "source": "virustotal",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "query_time_ms": 0,
                },
            }
        except requests.exceptions.Timeout:
            return {
                "status": "error",
                "error": {"code": -32005, "message": "VirusTotal ìš”ì²­ íƒ€ì„ì•„ì›ƒ"},
            }
        except Exception as e:
            logger.error(f"VirusTotal IP ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return {
                "status": "error",
                "error": {"code": -32000, "message": f"VirusTotal ì˜¤ë¥˜: {str(e)}"},
            }

    def _calculate_threat_level(self, stats: Dict[str, int]) -> str:
        """ìœ„í˜‘ ìˆ˜ì¤€ ê³„ì‚°"""
        malicious = stats.get("malicious", 0)
        suspicious = stats.get("suspicious", 0)

        if malicious >= 5:
            return "Critical"
        elif malicious >= 2:
            return "High"
        elif malicious + suspicious >= 5:
            return "Medium"
        elif malicious + suspicious > 0:
            return "Low"
        else:
            return "None"

    def _mock_domain_response(self, domain: str) -> Dict[str, Any]:
        """Mock ë„ë©”ì¸ ì‘ë‹µ"""
        return {
            "status": "success",
            "data": {
                "domain": domain,
                "threat_level": "Low",
                "detected_by": 0,
                "analysis_stats": {"malicious": 0, "suspicious": 0, "undetected": 89},
                "last_analysis_date": datetime.now(timezone.utc).isoformat(),
                "categories": {"Sophos": "legitimate", "Kaspersky": "legitimate"},
            },
            "metadata": {
                "source": "virustotal",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "query_time_ms": 245,
                "note": "Mock data in DEBUG_MODE",
            },
        }

    def _mock_ip_response(self, ip: str) -> Dict[str, Any]:
        """Mock IP ì‘ë‹µ"""
        return {
            "status": "success",
            "data": {
                "ip_address": ip,
                "country": "US",
                "asn": 15169,
                "organization": "Google LLC",
                "threat_level": "None",
                "detected_by": 0,
                "analysis_stats": {"malicious": 0, "suspicious": 0, "undetected": 89},
            },
            "metadata": {
                "source": "virustotal",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "query_time_ms": 198,
                "note": "Mock data in DEBUG_MODE",
            },
        }


# ============================================================================
# Phase 3: Client ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# ============================================================================

intelx_client = IntelligenceXClient(INTELX_API_KEY)
sherlock_client = SherlockClient()
playwright_client = PlaywrightClient()
vt_client = VTClient(VIRUSTOTAL_API_KEY)

# ============================================================================
# Phase 4: FastAPI ì•± ì´ˆê¸°í™”
# ============================================================================

app = FastAPI(
    title="OSINT Unified MCP Server",
    description="fastmcp ê¸°ë°˜ í†µí•© OSINT ì„œë²„ (ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸)",
    version="2.0.0",
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# Phase 5: Root & Health Endpoints
# ============================================================================


@app.get("/")
async def root():
    """ì„œë²„ ì •ë³´"""
    return {
        "name": "OSINT Unified MCP Server (fastmcp v2.0)",
        "version": "2.0.0",
        "description": "ë‹¨ì¼ /mcp ì—”ë“œí¬ì¸íŠ¸ë¥¼ í†µí•œ í†µí•© OSINT ì„œë²„",
        "tools_count": 7,
        "debug_mode": DEBUG_MODE,
        "status": "running",
        "api_configured": {
            "intelx": bool(INTELX_API_KEY),
            "virustotal": bool(VIRUSTOTAL_API_KEY),
            "shodan": bool(SHODAN_API_KEY),
        },
    }


@app.get("/health")
async def health():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "debug_mode": DEBUG_MODE,
        "tools_available": 7,
    }


@app.get("/mcp")
async def mcp_root():
    """MCP ê¸°ë³¸ ê²½ë¡œ ì•ˆë‚´"""
    return {
        "message": "ì´ MCP ì„œë²„ëŠ” POST ë°©ì‹ì˜ JSON-RPC ìš”ì²­ì„ `/mcp`ë¡œ ì „ì†¡í•´ ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.",
        "example_request": {
            "jsonrpc": "2.0",
            "id": 1,
            "method": "tools/call",
            "params": {"name": "<tool_name>", "arguments": {}},
        },
        "available_tools": [tool["name"] for tool in MCP_TOOLS],
        "documentation": "ê° ë„êµ¬ë³„ ì…ë ¥ ìŠ¤í‚¤ë§ˆëŠ” `tools/list` ì‘ë‹µì„ ì°¸ê³ í•˜ì„¸ìš”.",
    }


# ============================================================================
# Phase 6: ê¸°ì¡´ MCP ì—”ë“œí¬ì¸íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)
# ============================================================================

# MCP Tool ì •ì˜ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
MCP_TOOLS = [
    {
        "name": "search_intelligence_x",
        "description": "Intelligence Xì—ì„œ ë‹¤í¬ì›¹ ë° ìœ ì¶œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "term": {
                    "type": "string",
                    "description": "ê²€ìƒ‰í•  ì…€ë ‰í„° (ì˜ˆ: email@example.com)",
                },
                "maxresults": {
                    "type": "integer",
                    "description": "ìµœëŒ€ ê²°ê³¼ ìˆ˜",
                    "default": 300,
                },
            },
            "required": ["term"],
        },
    },
    {
        "name": "search_username_sherlock",
        "description": "Sherlockì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìëª…ì„ ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤. íŠ¹ì • ì‚¬ì´íŠ¸ë§Œ ê²€ìƒ‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "username": {
                    "type": "string",
                    "description": "ê²€ìƒ‰í•  ì‚¬ìš©ìëª… (ì˜ˆ: john_doe)",
                },
                "sites": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ê²€ìƒ‰í•  ì‚¬ì´íŠ¸ ëª©ë¡ (ì˜ˆ: ['github', 'twitter', 'reddit']. ìƒëµí•˜ë©´ ëª¨ë“  ì‚¬ì´íŠ¸ ê²€ìƒ‰)",
                },
                "timeout": {
                    "type": "integer",
                    "description": "ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ",
                    "default": 300,
                },
            },
            "required": ["username"],
        },
    },
    {
        "name": "analyze_url_playwright",
        "description": "Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ URLì„ ë¶„ì„í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "ë¶„ì„í•  URL",
                },
            },
            "required": ["url"],
        },
    },
    {
        "name": "check_virustotal_domain",
        "description": "VirusTotalì—ì„œ ë„ë©”ì¸ì˜ ìœ„í˜‘ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. (ì•…ì„±ë„ ì ìˆ˜, íƒì§€ ë²¤ë” ë“±)",
        "inputSchema": {
            "type": "object",
            "properties": {
                "domain": {
                    "type": "string",
                    "description": "ì¡°íšŒí•  ë„ë©”ì¸",
                },
            },
            "required": ["domain"],
        },
    },
    {
        "name": "check_virustotal_ip",
        "description": "VirusTotalì—ì„œ IP ì£¼ì†Œì˜ ìœ„í˜‘ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. (êµ­ê°€, ASN, ì•…ì„±ë„ ë“±)",
        "inputSchema": {
            "type": "object",
            "properties": {
                "ip_address": {
                    "type": "string",
                    "description": "ì¡°íšŒí•  IP ì£¼ì†Œ",
                },
            },
            "required": ["ip_address"],
        },
    },
    {
        "name": "crawl_and_analyze_url",
        "description": "Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ URLì„ ìë™ìœ¼ë¡œ í¬ë¡¤ë§í•˜ê³  ì§€ëŠ¥í˜• ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. í˜ì´ì§€ ëª©ì  íŒŒì•…, ì£¼ìš” ì •ë³´ ì¶”ì¶œ, ë³´ì•ˆ ìœ„í—˜ ê°ì§€, ê´€ë ¨ í˜ì´ì§€ ìë™ íƒìƒ‰",
        "inputSchema": {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "ì‹œì‘í•  URL",
                },
                "max_depth": {
                    "type": "integer",
                    "description": "í¬ë¡¤ë§ ê¹Šì´ (ê¸°ë³¸ê°’: 2)",
                    "default": 2,
                },
                "max_pages": {
                    "type": "integer",
                    "description": "ìµœëŒ€ ë°©ë¬¸ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 10)",
                    "default": 10,
                },
                "url_pattern": {
                    "type": "string",
                    "description": "í¬ë¡¤ë§í•  URL íŒ¨í„´ (ì •ê·œí‘œí˜„ì‹, ì˜ˆ: .*github.com.*)",
                },
                "analyze_content": {
                    "type": "boolean",
                    "description": "ì§€ëŠ¥í˜• ì½˜í…ì¸  ë¶„ì„ (ê¸°ë³¸ê°’: true)",
                    "default": True,
                },
                "timeout": {
                    "type": "integer",
                    "description": "ì „ì²´ í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 60)",
                    "default": 60,
                },
            },
            "required": ["url"],
        },
    },
]


@app.post("/mcp")
async def mcp_unified_endpoint(request: Request):
    """
    í†µí•© MCP ì—”ë“œí¬ì¸íŠ¸ (ëª¨ë“  OSINT ë„êµ¬)
    JSON-RPC 2.0 í”„ë¡œí† ì½œ ì§€ì›
    """
    try:
        body = await request.json()
        method = body.get("method")
        params = body.get("params", {})
        request_id = body.get("id", 1)

        logger.info(f"MCP ìš”ì²­: method={method}, id={request_id}")

        # ì´ˆê¸°í™” ë©”ì„œë“œ
        if method == "initialize":
            return {
                "jsonrpc": "2.0",
                "id": request_id,
                "result": {
                    "protocolVersion": "2024-11-05",
                    "capabilities": {"tools": {}},
                    "serverInfo": {
                        "name": "OSINT Unified MCP Server (fastmcp v2.0)",
                        "version": "2.0.0",
                    },
                },
            }

        # ì•Œë¦¼ ë©”ì„œë“œ
        elif method == "notifications/initialized":
            logger.info("í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return {"jsonrpc": "2.0", "id": request_id}

        # ë„êµ¬ ëª©ë¡ ë°˜í™˜
        elif method == "tools/list":
            return {
                "jsonrpc": "2.0",
                "id": request_id,
                "result": {"tools": MCP_TOOLS},
            }

        # ë„êµ¬ í˜¸ì¶œ
        elif method == "tools/call":
            tool_name = params.get("name")
            arguments = params.get("arguments", {})

            # Tool 1: search_intelligence_x
            if tool_name == "search_intelligence_x":
                start_time = time.time()
                search_req = SearchRequest(
                    term=arguments.get("term"),
                    maxresults=arguments.get("maxresults", 100),
                    buckets=arguments.get("buckets"),
                    datefrom=arguments.get("datefrom"),
                    dateto=arguments.get("dateto"),
                )

                result = intelx_client.search(search_req)
                execution_time = (time.time() - start_time) * 1000

                return {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "result": {
                        "content": [
                            {
                                "type": "text",
                                "text": json.dumps(
                                    {
                                        **result,
                                        "execution_time_ms": int(execution_time),
                                    },
                                    indent=2,
                                    ensure_ascii=False,
                                ),
                            }
                        ]
                    },
                }

            # Tool 2: search_username_sherlock
            elif tool_name == "search_username_sherlock":
                start_time = time.time()
                sherlock_req = SherlockSearchRequest(
                    username=arguments.get("username"),
                    sites=arguments.get("sites"),
                    timeout=arguments.get("timeout", 120),
                )

                result = sherlock_client.search(sherlock_req)
                execution_time = (time.time() - start_time) * 1000

                return {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "result": {
                        "content": [
                            {
                                "type": "text",
                                "text": json.dumps(
                                    {
                                        **result,
                                        "execution_time_ms": int(execution_time),
                                    },
                                    indent=2,
                                    ensure_ascii=False,
                                ),
                            }
                        ]
                    },
                }

            # Tool 3: analyze_url_playwright
            elif tool_name == "analyze_url_playwright":
                start_time = time.time()
                playwright_req = PlaywrightAnalyzeRequest(
                    url=arguments.get("url"),
                    extract_metadata=arguments.get("extract_metadata", True),
                    extract_text=arguments.get("extract_text", True),
                    extract_links=arguments.get("extract_links", True),
                    screenshot=arguments.get("screenshot", False),
                    wait_for_selector=arguments.get("wait_for_selector"),
                    timeout=arguments.get("timeout", 30),
                )

                result = await playwright_client.analyze(playwright_req)
                execution_time = (time.time() - start_time) * 1000

                return {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "result": {
                        "content": [
                            {
                                "type": "text",
                                "text": json.dumps(
                                    {
                                        **result,
                                        "execution_time_ms": int(execution_time),
                                    },
                                    indent=2,
                                    ensure_ascii=False,
                                ),
                            }
                        ]
                    },
                }

            # Tool 4: check_virustotal_domain
            elif tool_name == "check_virustotal_domain":
                start_time = time.time()
                result = vt_client.query_domain(arguments.get("domain"))
                execution_time = (time.time() - start_time) * 1000

                if result.get("status") == "error":
                    return {
                        "jsonrpc": "2.0",
                        "id": request_id,
                        "error": {
                            "code": result.get("error", {}).get("code", -32000),
                            "message": result.get("error", {}).get(
                                "message", "Unknown error"
                            ),
                        },
                    }

                return {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "result": {
                        "content": [
                            {
                                "type": "text",
                                "text": json.dumps(
                                    {
                                        **result,
                                        "execution_time_ms": int(execution_time),
                                    },
                                    indent=2,
                                    ensure_ascii=False,
                                ),
                            }
                        ]
                    },
                }

            # Tool 5: check_virustotal_ip
            elif tool_name == "check_virustotal_ip":
                start_time = time.time()
                result = vt_client.query_ip(arguments.get("ip_address"))
                execution_time = (time.time() - start_time) * 1000

                if result.get("status") == "error":
                    return {
                        "jsonrpc": "2.0",
                        "id": request_id,
                        "error": {
                            "code": result.get("error", {}).get("code", -32000),
                            "message": result.get("error", {}).get(
                                "message", "Unknown error"
                            ),
                        },
                    }

                return {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "result": {
                        "content": [
                            {
                                "type": "text",
                                "text": json.dumps(
                                    {
                                        **result,
                                        "execution_time_ms": int(execution_time),
                                    },
                                    indent=2,
                                    ensure_ascii=False,
                                ),
                            }
                        ]
                    },
                }

            # Tool 6: crawl_and_analyze_url
            elif tool_name == "crawl_and_analyze_url":
                start_time = time.time()
                crawl_req = PlaywrightCrawlRequest(
                    url=arguments.get("url"),
                    max_depth=arguments.get("max_depth", 2),
                    max_pages=arguments.get("max_pages", 10),
                    url_pattern=arguments.get("url_pattern"),
                    extract_text=arguments.get("extract_text", True),
                    extract_links=arguments.get("extract_links", True),
                    analyze_content=arguments.get("analyze_content", True),
                    timeout=arguments.get("timeout", 60),
                )

                result = await playwright_client.crawl(crawl_req)
                execution_time = (time.time() - start_time) * 1000

                return {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "result": {
                        "content": [
                            {
                                "type": "text",
                                "text": json.dumps(
                                    {
                                        **result,
                                        "execution_time_ms": int(execution_time),
                                    },
                                    indent=2,
                                    ensure_ascii=False,
                                ),
                            }
                        ]
                    },
                }

            else:
                return {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "error": {
                        "code": -32601,
                        "message": f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {tool_name}",
                    },
                }

        else:
            return {
                "jsonrpc": "2.0",
                "id": request_id,
                "error": {"code": -32601, "message": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë©”ì†Œë“œ: {method}"},
            }

    except Exception as e:
        logger.error(f"MCP ìš”ì²­ ì²˜ë¦¬ ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={
                "jsonrpc": "2.0",
                "id": body.get("id", 1) if "body" in locals() else 1,
                "error": {"code": -32603, "message": str(e)},
            },
        )


if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("PORT", "8000"))

    logger.info("=" * 70)
    logger.info("ğŸš€ OSINT Unified MCP Server (fastmcp v2.0)")
    logger.info("=" * 70)
    logger.info(f"í¬íŠ¸: {port}")
    logger.info(f"DEBUG ëª¨ë“œ: {DEBUG_MODE}")
    logger.info("")
    logger.info("âœ… í™œì„±í™”ëœ OSINT ë„êµ¬:")
    logger.info("   1ï¸âƒ£  search_intelligence_x - ë‹¤í¬ì›¹/ìœ ì¶œ ë°ì´í„° ê²€ìƒ‰")
    logger.info("   2ï¸âƒ£  search_username_sherlock - ì‚¬ìš©ìëª… ê²€ìƒ‰")
    logger.info("   3ï¸âƒ£  analyze_url_playwright - URL ë¶„ì„")
    logger.info("   4ï¸âƒ£  check_virustotal_domain - VirusTotal ë„ë©”ì¸ í™•ì¸")
    logger.info("   5ï¸âƒ£  check_virustotal_ip - VirusTotal IP í™•ì¸")
    logger.info("   6ï¸âƒ£  crawl_and_analyze_url - URL ìë™ í¬ë¡¤ë§ & ì§€ëŠ¥í˜• ë¶„ì„")
    logger.info("")
    logger.info("ğŸ“ ì—”ë“œí¬ì¸íŠ¸:")
    logger.info(f"   http://localhost:{port}/ (ì„œë²„ ì •ë³´)")
    logger.info(f"   http://localhost:{port}/health (í—¬ìŠ¤ ì²´í¬)")
    logger.info(f"   http://localhost:{port}/mcp (MCP JSON-RPC 2.0)")
    logger.info("=" * 70)

    uvicorn.run(app, host="0.0.0.0", port=port)
