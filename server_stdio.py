import os
import json
import time
import logging
import asyncio
import subprocess
import shutil
import base64
from typing import Any, Dict, List, Optional
from datetime import datetime, timezone

from dotenv import load_dotenv

try:
    from intelxapi import intelx
except ImportError:
    intelx = None
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import requests
from pydantic import BaseModel, Field

from fastmcp import FastMCP

# OSINT ë°ì´í„°ë² ì´ìŠ¤ ë° PDF ìƒì„± ëª¨ë“ˆ
from db_manager import OSINTDatabase
from pdf_generator import PDFGenerator

# ============================================================================
# Phase 0: ì´ˆê¸°í™” ë° í™˜ê²½ì„¤ì •
# ============================================================================

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# API ì„¤ì •
INTELX_API_KEY = os.getenv("INTELX_API_KEY", "")
VIRUSTOTAL_API_KEY = os.getenv("VIRUSTOTAL_API_KEY", "")
SHODAN_API_KEY = os.getenv("SHODAN_API_KEY", "")
HARVESTER_API_KEY = os.getenv("HARVESTER_API_KEY", "")
DEBUG_MODE = os.getenv("DEBUG_MODE", "false").lower() == "true"

# API í‚¤ ìœ íš¨ì„± í™•ì¸
if not INTELX_API_KEY and not DEBUG_MODE:
    logger.warning("Intelligence X API KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

if not VIRUSTOTAL_API_KEY and not DEBUG_MODE:
    logger.warning(
        "VirusTotal API KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (DEBUG_MODE=trueì¸ ê²½ìš° Mock ë°ì´í„° ì‚¬ìš©)"
    )

if DEBUG_MODE:
    logger.info("ğŸ”§ DEBUG_MODE í™œì„±í™” - Mock ë°ì´í„° ì‚¬ìš©")

# ë°ì´í„°ë² ì´ìŠ¤ ë° PDF ìƒì„±ê¸° ì´ˆê¸°í™”
osint_db = OSINTDatabase("db.csv")
pdf_generator = PDFGenerator("./pdfs")
logger.info("âœ… OSINT ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
logger.info("âœ… PDF ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

# ============================================================================
# Phase 1: Pydantic ëª¨ë¸
# ============================================================================


class SearchRequest(BaseModel):
    term: str = Field(..., description="ê²€ìƒ‰í•  ì…€ë ‰í„° (ì´ë©”ì¼, ë„ë©”ì¸, IP ë“±)")
    maxresults: int = Field(100, description="ìµœëŒ€ ê²°ê³¼ ìˆ˜")
    timeout: int = Field(5, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)")
    buckets: Optional[List[str]] = Field(None, description="ê²€ìƒ‰í•  ë²„í‚· ëª©ë¡")
    datefrom: Optional[str] = Field(None, description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    dateto: Optional[str] = Field(None, description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")


class SherlockSearchRequest(BaseModel):
    username: str = Field(..., description="ê²€ìƒ‰í•  ì‚¬ìš©ìëª…")
    sites: Optional[List[str]] = Field(
        None, description="ê²€ìƒ‰í•  ì‚¬ì´íŠ¸ ëª©ë¡ (ì˜ˆ: ['github', 'twitter'])"
    )
    timeout: int = Field(120, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 120ì´ˆ)")


class PlaywrightAnalyzeRequest(BaseModel):
    url: str = Field(..., description="ë¶„ì„í•  URL")
    extract_metadata: bool = Field(
        True, description="ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì œëª©, ì„¤ëª…, ì´ë¯¸ì§€)"
    )
    extract_text: bool = Field(True, description="í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    extract_links: bool = Field(True, description="ë§í¬ ëª©ë¡ ì¶”ì¶œ")
    screenshot: bool = Field(False, description="ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜")
    wait_for_selector: Optional[str] = Field(
        None, description="íŠ¹ì • ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°"
    )
    timeout: int = Field(30, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


class PlaywrightCrawlRequest(BaseModel):
    url: str = Field(..., description="ì‹œì‘í•  URL")
    max_depth: int = Field(2, description="í¬ë¡¤ë§ ê¹Šì´ (ê¸°ë³¸ê°’: 2)")
    max_pages: int = Field(10, description="ìµœëŒ€ ë°©ë¬¸ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 10)")
    url_pattern: Optional[str] = Field(
        None, description="í¬ë¡¤ë§í•  URL íŒ¨í„´ (ì •ê·œí‘œí˜„ì‹, ì˜ˆ: .*github.com.*)"
    )
    extract_text: bool = Field(True, description="í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    extract_links: bool = Field(True, description="ë§í¬ ì¶”ì¶œ")
    analyze_content: bool = Field(True, description="ì§€ëŠ¥í˜• ì½˜í…ì¸  ë¶„ì„")
    timeout: int = Field(60, description="ì „ì²´ í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


class ActionStep(BaseModel):
    """ê°œë³„ ìƒí˜¸ì‘ìš© ì•¡ì…˜"""

    action: str = Field(
        ...,
        description="ì•¡ì…˜ íƒ€ì…: click, type, fill, press, select, scroll, wait, screenshot, navigate",
    )
    selector: Optional[str] = Field(None, description="CSS ì…€ë ‰í„° (í•„ìš”í•œ ê²½ìš°)")
    value: Optional[str] = Field(None, description="ì…ë ¥ê°’ (type, fill, selectì— ì‚¬ìš©)")
    delay: Optional[int] = Field(
        0, description="ì•¡ì…˜ ì‹¤í–‰ í›„ ëŒ€ê¸° ì‹œê°„ (ë°€ë¦¬ì´ˆ)"
    )
    timeout: Optional[int] = Field(5000, description="ì•¡ì…˜ íƒ€ì„ì•„ì›ƒ (ë°€ë¦¬ì´ˆ)")
    description: Optional[str] = Field(None, description="ì•¡ì…˜ ì„¤ëª…")


class PlaywrightInteractionRequest(BaseModel):
    """Playwright ë™ì  ìƒí˜¸ì‘ìš© ìš”ì²­"""

    url: str = Field(..., description="ì‹œì‘ URL")
    actions: List[ActionStep] = Field(..., description="ì‹¤í–‰í•  ì•¡ì…˜ ì‹œí€€ìŠ¤")
    save_session: bool = Field(False, description="ì„¸ì…˜ ì €ì¥ (ì¿ í‚¤/ìŠ¤í† ë¦¬ì§€)")
    session_name: Optional[str] = Field(None, description="ì„¸ì…˜ ì´ë¦„")
    load_session: Optional[str] = Field(None, description="ë¡œë“œí•  ì„¸ì…˜ ì´ë¦„")
    screenshot_final: bool = Field(True, description="ìµœì¢… ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜")
    extract_data: bool = Field(
        True, description="ìµœì¢… í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ (í…ìŠ¤íŠ¸, ë§í¬ ë“±)"
    )
    headless: bool = Field(True, description="í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (ê¸°ë³¸ê°’: True)")
    timeout: int = Field(60, description="ì „ì²´ ì‘ì—… íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


class PlaywrightAutoExploreRequest(BaseModel):
    """ìë™ íƒìƒ‰ ìš”ì²­"""

    url: str = Field(..., description="ì‹œì‘ URL")
    goal: str = Field(
        ...,
        description="íƒìƒ‰ ëª©í‘œ (ì˜ˆ: 'ì—°ë½ì²˜ ì°¾ê¸°', 'SNS ë§í¬ ì°¾ê¸°', 'íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ í˜ì´ì§€ ì°¾ê¸°')",
    )
    max_interactions: int = Field(
        10, description="ìµœëŒ€ ìƒí˜¸ì‘ìš© íšŸìˆ˜ (í´ë¦­, í¼ ì œì¶œ ë“±)"
    )
    max_pages: int = Field(5, description="ìµœëŒ€ ë°©ë¬¸ í˜ì´ì§€ ìˆ˜")
    timeout: int = Field(120, description="ì „ì²´ íƒìƒ‰ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


class PlaywrightDeepAnalyzeRequest(BaseModel):
    """ì¬ê·€ì  URL ë¶„ì„ ìš”ì²­"""

    url: str = Field(..., description="ì‹œì‘ URL")
    max_depth: int = Field(2, description="ì¬ê·€ ê¹Šì´ (ê¸°ë³¸ê°’: 2)")
    max_urls: int = Field(20, description="ìµœëŒ€ ë¶„ì„ URL ìˆ˜ (ê¸°ë³¸ê°’: 20)")
    include_external: bool = Field(
        True, description="ì™¸ë¶€ ë„ë©”ì¸ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)"
    )
    check_threats: bool = Field(
        False, description="VirusTotalë¡œ ìœ„í˜‘ ì •ë³´ í™•ì¸ (ê¸°ë³¸ê°’: False, ì‹œê°„ ì†Œìš”)"
    )
    extract_emails: bool = Field(True, description="ì´ë©”ì¼ ì£¼ì†Œ ì¶”ì¶œ")
    extract_phones: bool = Field(True, description="ì „í™”ë²ˆí˜¸ ì¶”ì¶œ")
    extract_social: bool = Field(True, description="ì†Œì…œ ë¯¸ë””ì–´ ë§í¬ ì¶”ì¶œ")
    timeout_per_url: int = Field(30, description="URLë‹¹ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


class ThreatIntelRequest(BaseModel):
    query: str = Field(..., description="ì¡°íšŒí•  ëŒ€ìƒ (ë„ë©”ì¸ ë˜ëŠ” IP)")
    query_type: str = Field("domain", description="ì¡°íšŒ íƒ€ì…: domain ë˜ëŠ” ip")
    timeout: int = Field(10, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


# ============================================================================
# Phase 2: Client Classes (ê¸°ì¡´ êµ¬í˜„ ìœ ì§€)
# ============================================================================


class IntelligenceXClient:
    """Intelligence X ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, api_key: str):
        self.client = None
        self.debug_mode = DEBUG_MODE

        if api_key and intelx is not None:
            self.client = intelx(api_key)
            self.client.API_ROOT = "https://free.intelx.io"

    def search(self, search_request: SearchRequest) -> Dict[str, Any]:
        if self.debug_mode:
            logger.info(f"DEBUG MODE: Mock ë°ì´í„° ë°˜í™˜ (ê²€ìƒ‰ì–´: {search_request.term})")
            return {
                "records": [
                    {
                        "name": f"Mock Result 1 for {search_request.term}",
                        "description": "This is a mock result for testing purposes",
                        "date": datetime.now().isoformat(),
                        "media": 1,
                        "type": 1,
                        "added": datetime.now().isoformat(),
                        "storageid": "mock-storage-id-1",
                        "bucket": "mock-bucket",
                    },
                    {
                        "name": f"Mock Result 2 for {search_request.term}",
                        "description": "Second mock result for testing",
                        "date": "2025-10-25T12:00:00",
                        "media": 1,
                        "type": 1,
                        "added": "2025-10-25T14:30:00",
                        "storageid": "mock-storage-id-2",
                        "bucket": "mock-bucket",
                    },
                ]
            }

        try:
            if not self.client:
                raise ValueError("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            results = self.client.search(
                search_request.term,
                maxresults=search_request.maxresults,
                timeout=search_request.timeout,
            )
            return results
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise ValueError(f"Intelligence X API ì˜¤ë¥˜: {str(e)}")


class SherlockClient:
    """Sherlock ì‚¬ìš©ìëª… ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.debug_mode = DEBUG_MODE

    def search(self, search_request: SherlockSearchRequest) -> Dict[str, Any]:
        """Sherlockìœ¼ë¡œ ì‚¬ìš©ìëª… ê²€ìƒ‰"""
        if self.debug_mode:
            logger.info(
                f"DEBUG MODE: Sherlock Mock ë°ì´í„° ë°˜í™˜ (ì‚¬ìš©ìëª…: {search_request.username})"
            )
            return {
                "found": {
                    "github": {
                        "url": f"https://github.com/{search_request.username}",
                        "status": "found",
                    },
                    "twitter": {
                        "url": f"https://twitter.com/{search_request.username}",
                        "status": "found",
                    },
                },
                "not_found": ["instagram", "reddit"],
                "total_found": 2,
                "total_checked": 4,
            }

        try:
            # Sherlock ì ˆëŒ€ ê²½ë¡œ ì°¾ê¸°
            sherlock_path = shutil.which("sherlock")
            if not sherlock_path:
                raise FileNotFoundError(
                    "Sherlockì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install sherlock-project' ì‹¤í–‰í•˜ì„¸ìš”."
                )

            cmd = [sherlock_path, search_request.username, "--no-color", "--no-txt"]

            if search_request.sites:
                for site in search_request.sites:
                    cmd.extend(["--site", site])

            logger.info(f"Sherlock ê²€ìƒ‰ ì‹¤í–‰: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=search_request.timeout * len(search_request.sites or [100]),
            )

            found_accounts = {}
            lines = result.stdout.split("\n")

            for line in lines:
                if line.strip().startswith("[+]"):
                    parts = line.strip()[4:].split(": ", 1)
                    if len(parts) == 2:
                        site_name = parts[0].strip()
                        url = parts[1].strip()
                        found_accounts[site_name] = {"url": url, "status": "found"}

            total_found = len(found_accounts)

            return {
                "found": found_accounts,
                "total_found": total_found,
                "username": search_request.username,
                "timestamp": datetime.now().isoformat(),
                "status": "completed",
            }
        except subprocess.TimeoutExpired:
            logger.error(f"Sherlock íƒ€ì„ì•„ì›ƒ: {search_request.username}")
            raise TimeoutError("Sherlock ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            logger.error(f"Sherlock ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise ValueError(f"Sherlock ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")


class PlaywrightClient:
    """Playwright URL ë¶„ì„ ë° ìë™ í¬ë¡¤ë§ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.debug_mode = DEBUG_MODE
        self.visited_urls = set()
        self.crawl_results = []
        self.session_storage_dir = "./sessions"
        os.makedirs(self.session_storage_dir, exist_ok=True)

    async def analyze(
        self, analyze_request: PlaywrightAnalyzeRequest
    ) -> Dict[str, Any]:
        """Playwrightë¡œ URL ë¶„ì„"""
        if self.debug_mode:
            logger.info(
                f"DEBUG MODE: Playwright Mock ë°ì´í„° ë°˜í™˜ (URL: {analyze_request.url})"
            )
            return {
                "url": analyze_request.url,
                "metadata": {
                    "title": "Mock Page Title",
                    "description": "This is a mock page description",
                    "image": "https://example.com/image.jpg",
                },
                "text": "Mock page content...",
                "links": [
                    {"text": "Link 1", "href": "https://example.com/link1"},
                    {"text": "Link 2", "href": "https://example.com/link2"},
                ],
                "screenshot": None,
                "status": "completed",
            }

        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch()
                page = await browser.new_page()

                logger.info(f"Playwright í˜ì´ì§€ ë¡œë“œ: {analyze_request.url}")
                await page.goto(
                    analyze_request.url,
                    timeout=analyze_request.timeout * 1000,
                    wait_until="load",
                )

                if analyze_request.wait_for_selector:
                    await page.wait_for_selector(
                        analyze_request.wait_for_selector, timeout=5000
                    )

                result = {"url": analyze_request.url, "status": "completed"}

                if analyze_request.extract_metadata:
                    title = await page.title()

                    try:
                        meta_description = await page.locator(
                            'meta[name="description"]'
                        ).get_attribute("content", timeout=1000)
                    except:
                        meta_description = None

                    try:
                        meta_image = await page.locator(
                            'meta[property="og:image"]'
                        ).get_attribute("content", timeout=1000)
                    except:
                        meta_image = None

                    result["metadata"] = {
                        "title": title,
                        "description": meta_description or "",
                        "image": meta_image or "",
                    }

                if analyze_request.extract_text:
                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")
                    for script in soup(["script", "style"]):
                        script.decompose()
                    text = soup.get_text(separator="\n", strip=True)
                    result["text"] = text[:2000] if text else ""

                if analyze_request.extract_links:
                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")
                    links = []
                    for link in soup.find_all("a", href=True):
                        links.append(
                            {"text": link.get_text(strip=True), "href": link["href"]}
                        )
                    result["links"] = links[:50]

                if analyze_request.screenshot:
                    screenshot_bytes = await page.screenshot()
                    result["screenshot"] = base64.b64encode(screenshot_bytes).decode(
                        "utf-8"
                    )

                await browser.close()
                return result

        except Exception as e:
            logger.error(f"Playwright ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise ValueError(f"URL ë¶„ì„ ì˜¤ë¥˜: {str(e)}")

    def _analyze_content(self, html: str, text: str, url: str) -> Dict[str, Any]:
        """í˜ì´ì§€ ì½˜í…ì¸ ì˜ ì§€ëŠ¥í˜• ë¶„ì„"""
        import re

        analysis = {
            "page_purpose": self._detect_page_purpose(text, html, url),
            "key_information": self._extract_key_info(text, html),
            "potential_risks": self._detect_risks(text, html, url),
            "entities": self._extract_entities(text),
            "keywords": self._extract_keywords(text),
        }
        return analysis

    def _detect_page_purpose(self, text: str, html: str, url: str) -> str:
        """í˜ì´ì§€ì˜ ëª©ì  íŒŒì•…"""
        text_lower = text.lower()
        url_lower = url.lower()

        # í”„ë¡œí•„ í˜ì´ì§€ ê°ì§€
        if any(
            keyword in text_lower for keyword in ["profile", "about", "bio", "user"]
        ) or any(keyword in url_lower for keyword in ["profile", "user", "about"]):
            return "User/Profile Page"

        # ë¡œê·¸ì¸ í˜ì´ì§€
        if any(
            keyword in text_lower
            for keyword in ["login", "password", "username", "sign in"]
        ):
            return "Authentication/Login Page"

        # ìƒê±°ë˜ ì‚¬ì´íŠ¸
        if any(
            keyword in text_lower
            for keyword in ["price", "buy", "purchase", "cart", "checkout", "product"]
        ):
            return "E-commerce/Shopping Page"

        # ë¬¸ì„œ/ë¸”ë¡œê·¸
        if any(
            keyword in text_lower
            for keyword in ["article", "blog", "post", "author", "published"]
        ):
            return "Blog/Article Page"

        # ê²€ìƒ‰ ê²°ê³¼
        if any(
            keyword in text_lower for keyword in ["search result", "found", "matches"]
        ):
            return "Search Results Page"

        # API í˜ì´ì§€
        if "api" in url_lower or any(
            keyword in text_lower
            for keyword in ["endpoint", "request", "response", "json"]
        ):
            return "API/Technical Documentation"

        return "General Content Page"

    def _extract_key_info(self, text: str, html: str) -> Dict[str, Any]:
        """ì£¼ìš” ì •ë³´ ì¶”ì¶œ"""
        import re

        info = {}

        # ì´ë©”ì¼ ì¶”ì¶œ
        emails = set(
            re.findall(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", text)
        )
        if emails:
            info["emails"] = list(emails)[:5]  # ìµœëŒ€ 5ê°œ

        # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
        phones = set(re.findall(r"\b\d{3}[-.]?\d{3}[-.]?\d{4}\b", text))
        if phones:
            info["phone_numbers"] = list(phones)[:5]

        # ì†Œì…œ ë¯¸ë””ì–´ ë§í¬ ì¶”ì¶œ
        social_patterns = {
            "twitter": r"twitter\.com/[\w]+",
            "github": r"github\.com/[\w-]+",
            "linkedin": r"linkedin\.com/in/[\w-]+",
            "instagram": r"instagram\.com/[\w.]+",
            "facebook": r"facebook\.com/[\w.]+",
        }

        social_links = {}
        for platform, pattern in social_patterns.items():
            matches = re.findall(pattern, text)
            if matches:
                social_links[platform] = list(set(matches))[:3]

        if social_links:
            info["social_media"] = social_links

        # HTML ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´
        soup = BeautifulSoup(html, "html.parser")

        # Open Graph ì •ë³´
        og_title = soup.find("meta", property="og:title")
        og_description = soup.find("meta", property="og:description")

        if og_title:
            info["og_title"] = og_title.get("content")
        if og_description:
            info["og_description"] = og_description.get("content")

        # ì¡°ì§ëª… ì¶”ì¶œ ì‹œë„
        author = soup.find("meta", {"name": "author"})
        if author:
            info["author"] = author.get("content")

        return info

    def _detect_risks(self, text: str, html: str, url: str) -> List[str]:
        """ì ì¬ì  ë³´ì•ˆ ìœ„í—˜ ê°ì§€"""
        risks = []
        text_lower = text.lower()
        url_lower = url.lower()

        # í”¼ì‹± ì§•í›„
        if any(
            keyword in text_lower
            for keyword in [
                "verify account",
                "confirm identity",
                "update payment",
                "urgent action required",
            ]
        ):
            risks.append("Potential phishing indicators")

        # ì¸ì¦ ìš”êµ¬
        if "password" in text_lower or "login" in text_lower:
            risks.append("Authentication required - verify legitimacy")

        # ì˜ì‹¬ í™œë™
        if any(
            keyword in text_lower
            for keyword in [
                "limited time",
                "act now",
                "verify immediately",
                "confirm now",
            ]
        ):
            risks.append("High-pressure/urgency language detected")

        # ì™¸ë¶€ ìŠ¤í¬ë¦½íŠ¸ (XSS ê°€ëŠ¥ì„±)
        if "<script" in html and "src=" in html:
            script_count = html.count("<script")
            if script_count > 5:
                risks.append(f"Multiple external scripts detected ({script_count})")

        # HTTP vs HTTPS
        if url_lower.startswith("http://") and any(
            keyword in text_lower for keyword in ["password", "payment", "card"]
        ):
            risks.append("Sensitive data handling over insecure HTTP")

        # ìˆ¨ê²¨ì§„ í¼
        if "<form" in html and "style" in html.lower():
            if "display:none" in html.lower() or "visibility:hidden" in html.lower():
                risks.append("Hidden form elements detected")

        return risks if risks else ["No obvious security risks detected"]

    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜)"""
        import re

        entities = {}

        # URL ì¶”ì¶œ
        urls = re.findall(r'https?://[^\s<>"{}|\\^`\[\]]*', text)
        if urls:
            entities["urls"] = list(set(urls))[:10]

        # IP ì£¼ì†Œ ì¶”ì¶œ
        ips = re.findall(r"\b(?:\d{1,3}\.){3}\d{1,3}\b", text)
        if ips:
            entities["ip_addresses"] = list(set(ips))[:10]

        # ë„ë©”ì¸ ì¶”ì¶œ
        domains = re.findall(
            r"\b(?:[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?\.)+[a-z]{2,}\b", text.lower()
        )
        if domains:
            entities["domains"] = list(set(domains))[:10]

        return entities

    def _extract_keywords(self, text: str) -> List[str]:
        """ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ìˆœ ë¹ˆë„ ê¸°ë°˜)"""
        # ë¶ˆìš©ì–´ ì œì™¸
        stopwords = {
            "the",
            "a",
            "an",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "is",
            "was",
            "are",
            "be",
            "have",
            "has",
            "do",
            "does",
            "did",
            "this",
            "that",
            "these",
            "those",
            "i",
            "you",
            "he",
            "she",
            "it",
            "we",
            "they",
        }

        words = text.lower().split()
        # ê¸¸ì´ 4 ì´ìƒì˜ ë‹¨ì–´ë§Œ ê³ ë ¤
        filtered_words = [
            w for w in words if len(w) > 4 and w not in stopwords and w.isalpha()
        ]

        # ë¹ˆë„ ê³„ì‚°
        from collections import Counter

        word_freq = Counter(filtered_words)

        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
        keywords = [word for word, _ in word_freq.most_common(10)]
        return keywords

    async def crawl(self, crawl_request: PlaywrightCrawlRequest) -> Dict[str, Any]:
        """ìë™ í¬ë¡¤ë§ ë° ì§€ëŠ¥í˜• ë¶„ì„"""
        import re
        from urllib.parse import urljoin, urlparse

        if self.debug_mode:
            logger.info(
                f"DEBUG MODE: Crawler Mock ë°ì´í„° ë°˜í™˜ (URL: {crawl_request.url})"
            )
            return {
                "start_url": crawl_request.url,
                "pages_crawled": 3,
                "max_depth": crawl_request.max_depth,
                "summary": {
                    "primary_purpose": "Mock User Profile Page",
                    "key_findings": [
                        "4 repositories",
                        "Public profile",
                        "Active developer",
                    ],
                    "social_links": {"github": ["https://github.com/kjmkjmkj"]},
                    "risks": ["No obvious security risks detected"],
                },
                "pages": [
                    {
                        "url": crawl_request.url,
                        "title": "Mock Profile",
                        "purpose": "User/Profile Page",
                        "key_info": {
                            "social_media": {"github": ["https://github.com/kjmkjmkj"]}
                        },
                        "depth": 0,
                    }
                ],
                "status": "completed",
                "note": "Mock data in DEBUG_MODE",
            }

        try:
            self.visited_urls = set()
            self.crawl_results = []

            start_time = time.time()
            async with async_playwright() as p:
                browser = await p.chromium.launch()

                await self._crawl_recursive(
                    crawl_request.url, browser, crawl_request, depth=0
                )

                await browser.close()

            execution_time = time.time() - start_time

            # ë¶„ì„ ê²°ê³¼ ì¢…í•©
            summary = self._generate_summary(self.crawl_results, crawl_request.url)

            return {
                "start_url": crawl_request.url,
                "pages_crawled": len(self.crawl_results),
                "max_depth": crawl_request.max_depth,
                "summary": summary,
                "pages": self.crawl_results,
                "status": "completed",
                "execution_time_ms": int(execution_time * 1000),
            }

        except Exception as e:
            logger.error(f"Playwright í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise ValueError(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")

    async def _crawl_recursive(
        self, url: str, browser, crawl_request: PlaywrightCrawlRequest, depth: int
    ) -> None:
        """ì¬ê·€ì  í¬ë¡¤ë§"""
        import re
        from urllib.parse import urljoin, urlparse

        # ë°©ë¬¸ ì œí•œ í™•ì¸
        if url in self.visited_urls:
            return
        if len(self.crawl_results) >= crawl_request.max_pages:
            return
        if depth > crawl_request.max_depth:
            return

        # URL íŒ¨í„´ í™•ì¸
        if crawl_request.url_pattern:
            if not re.search(crawl_request.url_pattern, url):
                return

        # ê°™ì€ ë„ë©”ì¸ í™•ì¸
        start_domain = urlparse(crawl_request.url).netloc
        current_domain = urlparse(url).netloc
        if start_domain != current_domain:
            return

        self.visited_urls.add(url)

        try:
            page = await browser.new_page()
            logger.info(f"í¬ë¡¤ë§: {url} (ê¹Šì´: {depth})")

            await page.goto(
                url, timeout=crawl_request.timeout * 1000, wait_until="load"
            )

            # í˜ì´ì§€ ë¶„ì„
            html = await page.content()
            soup = BeautifulSoup(html, "html.parser")

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text(separator="\n", strip=True)

            # ë©”íƒ€ë°ì´í„°
            title = await page.title()

            # ë¶„ì„
            page_analysis = {}
            if crawl_request.analyze_content:
                page_analysis = self._analyze_content(html, text, url)

            # ë§í¬ ì¶”ì¶œ
            links = []
            if crawl_request.extract_links:
                for link in soup.find_all("a", href=True):
                    href = link["href"]
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    absolute_url = urljoin(url, href)
                    # í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±°
                    absolute_url = absolute_url.split("#")[0]
                    if absolute_url not in self.visited_urls:
                        links.append(absolute_url)

            # ê²°ê³¼ ì €ì¥
            self.crawl_results.append(
                {
                    "url": url,
                    "depth": depth,
                    "title": title,
                    "text": text[:1500] if crawl_request.extract_text else "",
                    **page_analysis,
                }
            )

            # ë‹¤ìŒ ë ˆë²¨ í¬ë¡¤ë§
            if depth < crawl_request.max_depth:
                for link in links[:5]:  # ê° í˜ì´ì§€ë‹¹ ìµœëŒ€ 5ê°œ ë§í¬ë§Œ ë”°ë¼ê°€ê¸°
                    if len(self.crawl_results) < crawl_request.max_pages:
                        await self._crawl_recursive(
                            link, browser, crawl_request, depth + 1
                        )

            await page.close()

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ({url}): {e}")

    def _generate_summary(self, results: List[Dict], start_url: str) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ê²°ê³¼ ì¢…í•© ë¶„ì„"""
        if not results:
            return {"status": "no_results"}

        # ì²« ë²ˆì§¸ í˜ì´ì§€ë¥¼ ì£¼ìš” ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ
        primary = results[0]

        summary = {
            "primary_purpose": primary.get("page_purpose", "Unknown"),
            "total_pages_analyzed": len(results),
            "key_findings": [],
            "all_entities": {
                "emails": set(),
                "social_media": {},
                "urls": set(),
                "domains": set(),
            },
            "risks": set(),
            "top_keywords": [],
        }

        # ëª¨ë“  í˜ì´ì§€ì—ì„œ ì •ë³´ í†µí•©
        for result in results:
            # ë¦¬ìŠ¤í¬ í†µí•©
            if "potential_risks" in result:
                summary["risks"].update(result["potential_risks"])

            # ì—”í‹°í‹° í†µí•©
            if "entities" in result:
                for entity_type, values in result["entities"].items():
                    if entity_type in summary["all_entities"]:
                        if isinstance(values, list):
                            summary["all_entities"][entity_type].update(values)

            # ì£¼ìš” ì •ë³´
            if "key_information" in result:
                if "emails" in result["key_information"]:
                    summary["all_entities"]["emails"].update(
                        result["key_information"]["emails"]
                    )
                if "social_media" in result["key_information"]:
                    summary["all_entities"]["social_media"].update(
                        result["key_information"]["social_media"]
                    )

            # í‚¤ì›Œë“œ
            if "keywords" in result:
                summary["top_keywords"].extend(result["keywords"][:3])

        # Setì„ Listë¡œ ë³€í™˜
        summary["all_entities"]["emails"] = list(summary["all_entities"]["emails"])[:10]
        summary["all_entities"]["urls"] = list(summary["all_entities"]["urls"])[:10]
        summary["all_entities"]["domains"] = list(summary["all_entities"]["domains"])[
            :10
        ]
        summary["risks"] = list(summary["risks"])

        # í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 10ê°œë§Œ
        from collections import Counter

        keyword_counts = Counter(summary["top_keywords"])
        summary["top_keywords"] = [word for word, _ in keyword_counts.most_common(10)]

        # ì£¼ìš” ë°œê²¬ì‚¬í•­
        if summary["all_entities"]["social_media"]:
            summary["key_findings"].append(
                f"Found social media links: {list(summary['all_entities']['social_media'].keys())}"
            )
        if summary["all_entities"]["emails"]:
            summary["key_findings"].append(
                f"Found {len(summary['all_entities']['emails'])} email addresses"
            )
        if len(results) > 1:
            summary["key_findings"].append(f"Crawled {len(results)} related pages")
        if summary["all_entities"]["urls"]:
            summary["key_findings"].append(
                f"Found {len(summary['all_entities']['urls'])} external URLs"
            )

        return summary

    async def interact(
        self, interaction_request: PlaywrightInteractionRequest
    ) -> Dict[str, Any]:
        """ë™ì  ìƒí˜¸ì‘ìš© ì‹¤í–‰"""
        if self.debug_mode:
            logger.info(
                f"DEBUG MODE: Interaction Mock ë°ì´í„° ë°˜í™˜ (URL: {interaction_request.url})"
            )
            return {
                "url": interaction_request.url,
                "actions_executed": len(interaction_request.actions),
                "actions": [
                    {"action": step.action, "status": "success (mock)"}
                    for step in interaction_request.actions
                ],
                "final_url": interaction_request.url,
                "screenshot": None,
                "status": "completed (mock)",
            }

        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=interaction_request.headless
                )
                context = await browser.new_context()

                # ì„¸ì…˜ ë¡œë“œ
                if interaction_request.load_session:
                    await self._load_session(context, interaction_request.load_session)

                page = await context.new_page()
                logger.info(f"ìƒí˜¸ì‘ìš© ì‹œì‘: {interaction_request.url}")
                await page.goto(interaction_request.url, wait_until="load")

                action_results = []

                # ì•¡ì…˜ ì‹œí€€ìŠ¤ ì‹¤í–‰
                for idx, action_step in enumerate(interaction_request.actions):
                    logger.info(
                        f"ì•¡ì…˜ {idx + 1}/{len(interaction_request.actions)}: {action_step.action}"
                    )
                    try:
                        result = await self._execute_action(page, action_step)
                        action_results.append(
                            {
                                "step": idx + 1,
                                "action": action_step.action,
                                "selector": action_step.selector,
                                "description": action_step.description,
                                "status": "success",
                                "result": result,
                            }
                        )

                        # ë”œë ˆì´
                        if action_step.delay and action_step.delay > 0:
                            await page.wait_for_timeout(action_step.delay)

                    except Exception as e:
                        logger.error(f"ì•¡ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {action_step.action} - {e}")
                        action_results.append(
                            {
                                "step": idx + 1,
                                "action": action_step.action,
                                "selector": action_step.selector,
                                "status": "failed",
                                "error": str(e),
                            }
                        )

                # ìµœì¢… ë°ì´í„° ì¶”ì¶œ
                final_data = {}
                if interaction_request.extract_data:
                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")

                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    for script in soup(["script", "style"]):
                        script.decompose()
                    text = soup.get_text(separator="\n", strip=True)
                    final_data["text"] = text[:2000] if text else ""

                    # ë§í¬ ì¶”ì¶œ
                    links = []
                    for link in soup.find_all("a", href=True):
                        links.append(
                            {"text": link.get_text(strip=True), "href": link["href"]}
                        )
                    final_data["links"] = links[:50]

                    # ë©”íƒ€ë°ì´í„°
                    final_data["title"] = await page.title()
                    final_data["url"] = page.url

                # ìµœì¢… ìŠ¤í¬ë¦°ìƒ·
                screenshot_b64 = None
                if interaction_request.screenshot_final:
                    screenshot_bytes = await page.screenshot(full_page=True)
                    screenshot_b64 = base64.b64encode(screenshot_bytes).decode("utf-8")

                # ì„¸ì…˜ ì €ì¥
                if interaction_request.save_session and interaction_request.session_name:
                    await self._save_session(context, interaction_request.session_name)

                await browser.close()

                return {
                    "url": interaction_request.url,
                    "final_url": page.url,
                    "actions_executed": len(action_results),
                    "actions": action_results,
                    "data": final_data,
                    "screenshot": screenshot_b64,
                    "status": "completed",
                }

        except Exception as e:
            logger.error(f"ìƒí˜¸ì‘ìš© ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ìƒí˜¸ì‘ìš© ì˜¤ë¥˜: {str(e)}")

    async def _execute_action(self, page, action_step: ActionStep) -> Any:
        """ê°œë³„ ì•¡ì…˜ ì‹¤í–‰"""
        action = action_step.action.lower()
        selector = action_step.selector
        value = action_step.value
        timeout = action_step.timeout or 5000

        if action == "click":
            if not selector:
                raise ValueError("click ì•¡ì…˜ì—ëŠ” selectorê°€ í•„ìš”í•©ë‹ˆë‹¤")
            await page.click(selector, timeout=timeout)
            return f"í´ë¦­ ì™„ë£Œ: {selector}"

        elif action == "type":
            if not selector or not value:
                raise ValueError("type ì•¡ì…˜ì—ëŠ” selectorì™€ valueê°€ í•„ìš”í•©ë‹ˆë‹¤")
            await page.type(selector, value, timeout=timeout)
            return f"ì…ë ¥ ì™„ë£Œ: {selector} = {value}"

        elif action == "fill":
            if not selector or not value:
                raise ValueError("fill ì•¡ì…˜ì—ëŠ” selectorì™€ valueê°€ í•„ìš”í•©ë‹ˆë‹¤")
            await page.fill(selector, value, timeout=timeout)
            return f"ì±„ìš°ê¸° ì™„ë£Œ: {selector} = {value}"

        elif action == "press":
            if not value:
                raise ValueError("press ì•¡ì…˜ì—ëŠ” value(í‚¤)ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            await page.keyboard.press(value)
            return f"í‚¤ ì…ë ¥: {value}"

        elif action == "select":
            if not selector or not value:
                raise ValueError("select ì•¡ì…˜ì—ëŠ” selectorì™€ valueê°€ í•„ìš”í•©ë‹ˆë‹¤")
            await page.select_option(selector, value, timeout=timeout)
            return f"ì„ íƒ ì™„ë£Œ: {selector} = {value}"

        elif action == "scroll":
            # valueì— í”½ì…€ ìˆ˜ ë˜ëŠ” "bottom" ì§€ì •
            if value == "bottom":
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                return "í˜ì´ì§€ í•˜ë‹¨ê¹Œì§€ ìŠ¤í¬ë¡¤"
            elif value:
                await page.evaluate(f"window.scrollBy(0, {value})")
                return f"{value}px ìŠ¤í¬ë¡¤"
            else:
                await page.evaluate("window.scrollBy(0, 500)")
                return "500px ìŠ¤í¬ë¡¤"

        elif action == "wait":
            # valueì— ë°€ë¦¬ì´ˆ ì§€ì •
            wait_time = int(value) if value else 1000
            await page.wait_for_timeout(wait_time)
            return f"{wait_time}ms ëŒ€ê¸°"

        elif action == "wait_for_selector":
            if not selector:
                raise ValueError("wait_for_selector ì•¡ì…˜ì—ëŠ” selectorê°€ í•„ìš”í•©ë‹ˆë‹¤")
            await page.wait_for_selector(selector, timeout=timeout)
            return f"ìš”ì†Œ ë¡œë“œ ëŒ€ê¸° ì™„ë£Œ: {selector}"

        elif action == "screenshot":
            # valueì— íŒŒì¼ëª… ì§€ì • ê°€ëŠ¥
            filename = value or f"screenshot_{int(time.time())}.png"
            screenshot_bytes = await page.screenshot()
            screenshot_b64 = base64.b64encode(screenshot_bytes).decode("utf-8")
            return {"screenshot": screenshot_b64, "filename": filename}

        elif action == "navigate":
            if not value:
                raise ValueError("navigate ì•¡ì…˜ì—ëŠ” value(URL)ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            await page.goto(value, wait_until="load")
            return f"í˜ì´ì§€ ì´ë™: {value}"

        elif action == "hover":
            if not selector:
                raise ValueError("hover ì•¡ì…˜ì—ëŠ” selectorê°€ í•„ìš”í•©ë‹ˆë‹¤")
            await page.hover(selector, timeout=timeout)
            return f"ë§ˆìš°ìŠ¤ ì˜¤ë²„: {selector}"

        elif action == "check":
            if not selector:
                raise ValueError("check ì•¡ì…˜ì—ëŠ” selectorê°€ í•„ìš”í•©ë‹ˆë‹¤")
            await page.check(selector, timeout=timeout)
            return f"ì²´í¬ë°•ìŠ¤ ì„ íƒ: {selector}"

        elif action == "uncheck":
            if not selector:
                raise ValueError("uncheck ì•¡ì…˜ì—ëŠ” selectorê°€ í•„ìš”í•©ë‹ˆë‹¤")
            await page.uncheck(selector, timeout=timeout)
            return f"ì²´í¬ë°•ìŠ¤ í•´ì œ: {selector}"

        elif action == "get_text":
            if not selector:
                raise ValueError("get_text ì•¡ì…˜ì—ëŠ” selectorê°€ í•„ìš”í•©ë‹ˆë‹¤")
            text = await page.text_content(selector, timeout=timeout)
            return {"text": text}

        elif action == "get_attribute":
            if not selector or not value:
                raise ValueError(
                    "get_attribute ì•¡ì…˜ì—ëŠ” selectorì™€ value(ì†ì„±ëª…)ê°€ í•„ìš”í•©ë‹ˆë‹¤"
                )
            attr = await page.get_attribute(selector, value, timeout=timeout)
            return {"attribute": value, "value": attr}

        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•¡ì…˜: {action}")

    async def _save_session(self, context, session_name: str) -> None:
        """ì„¸ì…˜ ì €ì¥ (ì¿ í‚¤ + ë¡œì»¬ìŠ¤í† ë¦¬ì§€)"""
        session_path = os.path.join(self.session_storage_dir, f"{session_name}.json")

        # ì¿ í‚¤ ì €ì¥
        cookies = await context.cookies()

        # ë¡œì»¬ìŠ¤í† ë¦¬ì§€ëŠ” í˜ì´ì§€ë³„ë¡œ ì €ì¥í•´ì•¼ í•¨
        # ê°„ë‹¨í•˜ê²Œ ì¿ í‚¤ë§Œ ì €ì¥
        session_data = {"cookies": cookies, "timestamp": datetime.now().isoformat()}

        with open(session_path, "w") as f:
            json.dump(session_data, f, indent=2)

        logger.info(f"ì„¸ì…˜ ì €ì¥ ì™„ë£Œ: {session_path}")

    async def _load_session(self, context, session_name: str) -> None:
        """ì„¸ì…˜ ë¡œë“œ"""
        session_path = os.path.join(self.session_storage_dir, f"{session_name}.json")

        if not os.path.exists(session_path):
            logger.warning(f"ì„¸ì…˜ íŒŒì¼ ì—†ìŒ: {session_path}")
            return

        with open(session_path, "r") as f:
            session_data = json.load(f)

        # ì¿ í‚¤ ë³µì›
        if "cookies" in session_data:
            await context.add_cookies(session_data["cookies"])

        logger.info(f"ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ: {session_path}")

    async def auto_explore(
        self, explore_request: PlaywrightAutoExploreRequest
    ) -> Dict[str, Any]:
        """ìë™ íƒìƒ‰ - ëª©í‘œ ê¸°ë°˜ ì§€ëŠ¥í˜• íƒìƒ‰"""
        if self.debug_mode:
            logger.info(
                f"DEBUG MODE: Auto-explore Mock ë°ì´í„° ë°˜í™˜ (URL: {explore_request.url})"
            )
            return {
                "url": explore_request.url,
                "goal": explore_request.goal,
                "pages_visited": 3,
                "interactions": 5,
                "findings": [
                    "Found contact form",
                    "Found social media links: Twitter, GitHub",
                ],
                "status": "completed (mock)",
            }

        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch()
                page = await browser.new_page()

                logger.info(
                    f"ìë™ íƒìƒ‰ ì‹œì‘: {explore_request.url} (ëª©í‘œ: {explore_request.goal})"
                )
                await page.goto(explore_request.url, wait_until="load")

                interactions = []
                findings = []
                pages_visited = [explore_request.url]

                # ëª©í‘œ í‚¤ì›Œë“œ ì¶”ì¶œ
                goal_keywords = self._extract_goal_keywords(explore_request.goal)

                for i in range(explore_request.max_interactions):
                    if len(pages_visited) >= explore_request.max_pages:
                        break

                    # í˜ì´ì§€ ë¶„ì„
                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")

                    # ëª©í‘œì™€ ê´€ë ¨ëœ ìš”ì†Œ ì°¾ê¸°
                    relevant_elements = await self._find_relevant_elements(
                        page, soup, goal_keywords
                    )

                    if not relevant_elements:
                        logger.info("ë” ì´ìƒ ê´€ë ¨ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        break

                    # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ìš”ì†Œ ì„ íƒ
                    element_info = relevant_elements[0]

                    # ìƒí˜¸ì‘ìš© ì‹œë„
                    try:
                        interaction_result = await self._interact_with_element(
                            page, element_info
                        )
                        interactions.append(interaction_result)

                        # ìƒˆë¡œìš´ ì •ë³´ ë°œê²¬
                        new_findings = await self._check_for_findings(
                            page, goal_keywords
                        )
                        findings.extend(new_findings)

                        # URL ë³€ê²½ í™•ì¸
                        current_url = page.url
                        if current_url not in pages_visited:
                            pages_visited.append(current_url)
                            logger.info(f"ìƒˆ í˜ì´ì§€ ë°©ë¬¸: {current_url}")

                    except Exception as e:
                        logger.error(f"ìƒí˜¸ì‘ìš© ì‹¤íŒ¨: {e}")
                        continue

                await browser.close()

                return {
                    "url": explore_request.url,
                    "goal": explore_request.goal,
                    "pages_visited": len(pages_visited),
                    "pages": pages_visited,
                    "interactions_count": len(interactions),
                    "interactions": interactions,
                    "findings": list(set(findings)),
                    "status": "completed",
                }

        except Exception as e:
            logger.error(f"ìë™ íƒìƒ‰ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ìë™ íƒìƒ‰ ì˜¤ë¥˜: {str(e)}")

    def _extract_goal_keywords(self, goal: str) -> List[str]:
        """ëª©í‘œì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = []

        goal_lower = goal.lower()

        # ì—°ë½ì²˜ ê´€ë ¨
        if any(
            word in goal_lower
            for word in ["ì—°ë½", "contact", "email", "ì´ë©”ì¼", "phone", "ì „í™”"]
        ):
            keywords.extend(["contact", "email", "phone", "tel", "ì—°ë½"])

        # SNS ê´€ë ¨
        if any(word in goal_lower for word in ["sns", "social", "ì†Œì…œ"]):
            keywords.extend(
                ["twitter", "facebook", "instagram", "linkedin", "github", "social"]
            )

        # ê¸°ë³¸ì ìœ¼ë¡œ goalì˜ ë‹¨ì–´ë“¤ ì¶”ê°€
        words = goal_lower.split()
        keywords.extend(words)

        return list(set(keywords))

    async def _find_relevant_elements(
        self, page, soup, keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """ê´€ë ¨ ìš”ì†Œ ì°¾ê¸°"""
        relevant = []

        # ë§í¬ ì°¾ê¸°
        for link in soup.find_all("a", href=True):
            text = link.get_text(strip=True).lower()
            href = link["href"].lower()

            # í‚¤ì›Œë“œ ë§¤ì¹­
            relevance_score = sum(
                1 for keyword in keywords if keyword in text or keyword in href
            )

            if relevance_score > 0:
                # CSS ì…€ë ‰í„° ìƒì„± ì‹œë„
                selector = None
                if link.get("id"):
                    selector = f"#{link['id']}"
                elif link.get("class"):
                    classes = " ".join(link["class"])
                    selector = f"a.{link['class'][0]}"

                if not selector:
                    # hrefë¡œ ì°¾ê¸°
                    selector = f'a[href="{link["href"]}"]'

                relevant.append(
                    {
                        "type": "link",
                        "text": link.get_text(strip=True),
                        "href": link["href"],
                        "selector": selector,
                        "relevance": relevance_score,
                    }
                )

        # ë²„íŠ¼ ì°¾ê¸°
        for button in soup.find_all(["button", "input"]):
            if button.name == "input" and button.get("type") not in [
                "submit",
                "button",
            ]:
                continue

            text = button.get_text(strip=True).lower()
            value = (button.get("value") or "").lower()

            relevance_score = sum(
                1 for keyword in keywords if keyword in text or keyword in value
            )

            if relevance_score > 0:
                selector = None
                if button.get("id"):
                    selector = f"#{button['id']}"
                elif button.get("class"):
                    selector = f"{button.name}.{button['class'][0]}"

                relevant.append(
                    {
                        "type": "button",
                        "text": text or value,
                        "selector": selector,
                        "relevance": relevance_score,
                    }
                )

        # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
        relevant.sort(key=lambda x: x["relevance"], reverse=True)

        return relevant[:5]  # ìƒìœ„ 5ê°œë§Œ

    async def _interact_with_element(
        self, page, element_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ìš”ì†Œì™€ ìƒí˜¸ì‘ìš©"""
        element_type = element_info["type"]
        selector = element_info["selector"]

        if element_type == "link":
            await page.click(selector)
            await page.wait_for_load_state("load")
            return {
                "action": "click_link",
                "text": element_info["text"],
                "href": element_info["href"],
            }

        elif element_type == "button":
            await page.click(selector)
            await page.wait_for_timeout(1000)
            return {
                "action": "click_button",
                "text": element_info["text"],
            }

        return {"action": "unknown"}

    async def _check_for_findings(
        self, page, keywords: List[str]
    ) -> List[str]:
        """ìƒˆë¡œìš´ ë°œê²¬ì‚¬í•­ í™•ì¸"""
        findings = []
        html = await page.content()
        soup = BeautifulSoup(html, "html.parser")

        # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
        text = soup.get_text().lower()

        for keyword in keywords:
            if keyword in text:
                findings.append(f"Found keyword: {keyword}")

        # ì´ë©”ì¼ ì°¾ê¸°
        import re

        emails = re.findall(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", text)
        if emails:
            findings.append(f"Found {len(set(emails))} email addresses")

        # ì „í™”ë²ˆí˜¸ ì°¾ê¸°
        phones = re.findall(r"\b\d{3}[-.]?\d{3}[-.]?\d{4}\b", text)
        if phones:
            findings.append(f"Found {len(set(phones))} phone numbers")

        return findings

    async def deep_analyze(
        self, deep_request: PlaywrightDeepAnalyzeRequest
    ) -> Dict[str, Any]:
        """ì¬ê·€ì  URL ë¶„ì„ - URLì„ ë¶„ì„í•˜ê³  ë°œê²¬ëœ ëª¨ë“  URLë„ ì¬ê·€ì ìœ¼ë¡œ ë¶„ì„"""
        if self.debug_mode:
            logger.info(
                f"DEBUG MODE: Deep analyze Mock ë°ì´í„° ë°˜í™˜ (URL: {deep_request.url})"
            )
            return {
                "start_url": deep_request.url,
                "total_urls_analyzed": 5,
                "urls": [
                    {
                        "url": deep_request.url,
                        "depth": 0,
                        "status": "success",
                        "title": "Mock Page",
                    }
                ],
                "relationships": [],
                "summary": {"total_emails": 3, "total_threats": 0},
                "status": "completed (mock)",
            }

        try:
            import re
            from urllib.parse import urljoin, urlparse

            async with async_playwright() as p:
                browser = await p.chromium.launch()
                context = await browser.new_context()

                # ë¶„ì„ ë°ì´í„° ì €ì¥
                analyzed_urls = {}  # url -> data
                url_queue = [(deep_request.url, 0, None)]  # (url, depth, parent)
                visited = set()
                relationships = []

                start_domain = urlparse(deep_request.url).netloc

                logger.info(f"ì¬ê·€ì  URL ë¶„ì„ ì‹œì‘: {deep_request.url}")

                while url_queue and len(analyzed_urls) < deep_request.max_urls:
                    current_url, depth, parent_url = url_queue.pop(0)

                    # ì¤‘ë³µ í™•ì¸
                    if current_url in visited:
                        continue

                    # ê¹Šì´ ì œí•œ
                    if depth > deep_request.max_depth:
                        continue

                    # ì™¸ë¶€ ë„ë©”ì¸ í•„í„°ë§
                    if not deep_request.include_external:
                        current_domain = urlparse(current_url).netloc
                        if current_domain != start_domain:
                            continue

                    visited.add(current_url)

                    # URL ê´€ê³„ ì €ì¥
                    if parent_url:
                        relationships.append(
                            {
                                "parent": parent_url,
                                "child": current_url,
                                "depth": depth,
                            }
                        )

                    logger.info(
                        f"ë¶„ì„ ì¤‘ ({len(analyzed_urls) + 1}/{deep_request.max_urls}): {current_url} (ê¹Šì´: {depth})"
                    )

                    # URL ë¶„ì„
                    url_data = await self._analyze_single_url(
                        browser, context, current_url, depth, parent_url, deep_request
                    )

                    analyzed_urls[current_url] = url_data

                    # ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ëœ ê²½ìš°ì—ë§Œ í•˜ìœ„ URL ì¶”ê°€
                    if url_data["status"] == "success" and "discovered_urls" in url_data:
                        for discovered_url in url_data["discovered_urls"][:10]:  # ìµœëŒ€ 10ê°œë§Œ
                            if discovered_url not in visited:
                                url_queue.append((discovered_url, depth + 1, current_url))

                await browser.close()

                # ìš”ì•½ ìƒì„±
                summary = self._generate_deep_analysis_summary(
                    analyzed_urls, relationships, deep_request
                )

                return {
                    "start_url": deep_request.url,
                    "total_urls_analyzed": len(analyzed_urls),
                    "max_depth_reached": max(
                        (data["depth"] for data in analyzed_urls.values()), default=0
                    ),
                    "urls": list(analyzed_urls.values()),
                    "relationships": relationships,
                    "summary": summary,
                    "status": "completed",
                }

        except Exception as e:
            logger.error(f"ì¬ê·€ì  URL ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ì¬ê·€ì  URL ë¶„ì„ ì˜¤ë¥˜: {str(e)}")

    async def _analyze_single_url(
        self,
        browser,
        context,
        url: str,
        depth: int,
        parent_url: Optional[str],
        deep_request: PlaywrightDeepAnalyzeRequest,
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ URL ë¶„ì„"""
        import re
        from urllib.parse import urljoin, urlparse

        result = {
            "url": url,
            "depth": depth,
            "parent_url": parent_url,
            "domain": urlparse(url).netloc,
            "status": "pending",
        }

        try:
            page = await context.new_page()
            await page.goto(
                url, timeout=deep_request.timeout_per_url * 1000, wait_until="load"
            )

            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
            result["title"] = await page.title()

            # HTML ê°€ì ¸ì˜¤ê¸°
            html = await page.content()
            soup = BeautifulSoup(html, "html.parser")

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text(separator="\n", strip=True)
            result["text_preview"] = text[:500] if text else ""

            # ë©”íƒ€ ì„¤ëª…
            meta_desc = soup.find("meta", {"name": "description"})
            if meta_desc:
                result["description"] = meta_desc.get("content", "")

            # ì´ë©”ì¼ ì¶”ì¶œ
            if deep_request.extract_emails:
                emails = set(
                    re.findall(
                        r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
                        text,
                    )
                )
                if emails:
                    result["emails"] = list(emails)[:10]

            # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
            if deep_request.extract_phones:
                phones = set(re.findall(r"\b\d{3}[-.]?\d{3}[-.]?\d{4}\b", text))
                if phones:
                    result["phones"] = list(phones)[:10]

            # ì†Œì…œ ë¯¸ë””ì–´ ë§í¬ ì¶”ì¶œ
            if deep_request.extract_social:
                social_patterns = {
                    "twitter": r"(?:twitter\.com|x\.com)/[\w]+",
                    "github": r"github\.com/[\w-]+",
                    "linkedin": r"linkedin\.com/(?:in|company)/[\w-]+",
                    "instagram": r"instagram\.com/[\w.]+",
                    "facebook": r"facebook\.com/[\w.]+",
                    "youtube": r"youtube\.com/(?:@|channel|c)/[\w-]+",
                }

                social_links = {}
                for platform, pattern in social_patterns.items():
                    matches = re.findall(pattern, text.lower())
                    if matches:
                        social_links[platform] = list(set(matches))[:3]

                if social_links:
                    result["social_media"] = social_links

            # URL ì¶”ì¶œ (ë§í¬ + í…ìŠ¤íŠ¸)
            discovered_urls = set()

            # HTML ë§í¬ì—ì„œ ì¶”ì¶œ
            for link in soup.find_all("a", href=True):
                href = link["href"]
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                absolute_url = urljoin(url, href)
                # í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±°
                absolute_url = absolute_url.split("#")[0]
                # http/httpsë§Œ
                if absolute_url.startswith(("http://", "https://")):
                    discovered_urls.add(absolute_url)

            # í…ìŠ¤íŠ¸ì—ì„œ URL ì¶”ì¶œ
            text_urls = re.findall(r'https?://[^\s<>"{}|\\^`\[\]]+', text)
            for text_url in text_urls:
                # ì •ë¦¬
                text_url = text_url.rstrip(".,;:)")
                discovered_urls.add(text_url)

            result["discovered_urls"] = list(discovered_urls)[:50]  # ìµœëŒ€ 50ê°œ
            result["url_count"] = len(discovered_urls)

            # ìœ„í˜‘ ì •ë³´ í™•ì¸ (ì„ íƒì )
            if deep_request.check_threats:
                domain = urlparse(url).netloc
                threat_result = vt_client.query_domain(domain)
                if threat_result.get("status") == "success":
                    threat_data = threat_result.get("data", {})
                    result["threat_info"] = {
                        "threat_level": threat_data.get("threat_level", "Unknown"),
                        "detected_by": threat_data.get("detected_by", 0),
                    }

            result["status"] = "success"
            await page.close()

        except Exception as e:
            logger.error(f"URL ë¶„ì„ ì‹¤íŒ¨ ({url}): {e}")
            result["status"] = "failed"
            result["error"] = str(e)

        return result

    def _generate_deep_analysis_summary(
        self,
        analyzed_urls: Dict[str, Dict],
        relationships: List[Dict],
        deep_request: PlaywrightDeepAnalyzeRequest,
    ) -> Dict[str, Any]:
        """ì¬ê·€ì  ë¶„ì„ ìš”ì•½ ìƒì„±"""
        summary = {
            "total_urls": len(analyzed_urls),
            "successful": sum(
                1 for data in analyzed_urls.values() if data["status"] == "success"
            ),
            "failed": sum(
                1 for data in analyzed_urls.values() if data["status"] == "failed"
            ),
        }

        # ë„ë©”ì¸ ë¶„í¬
        domains = {}
        for data in analyzed_urls.values():
            domain = data.get("domain", "unknown")
            domains[domain] = domains.get(domain, 0) + 1
        summary["domains"] = domains

        # ì´ë©”ì¼ ìˆ˜ì§‘
        all_emails = set()
        for data in analyzed_urls.values():
            if "emails" in data:
                all_emails.update(data["emails"])
        summary["total_emails_found"] = len(all_emails)
        summary["emails"] = list(all_emails)[:20]  # ìµœëŒ€ 20ê°œ

        # ì „í™”ë²ˆí˜¸ ìˆ˜ì§‘
        all_phones = set()
        for data in analyzed_urls.values():
            if "phones" in data:
                all_phones.update(data["phones"])
        summary["total_phones_found"] = len(all_phones)
        summary["phones"] = list(all_phones)[:20]

        # ì†Œì…œ ë¯¸ë””ì–´ í†µí•©
        all_social = {}
        for data in analyzed_urls.values():
            if "social_media" in data:
                for platform, links in data["social_media"].items():
                    if platform not in all_social:
                        all_social[platform] = set()
                    all_social[platform].update(links)

        summary["social_media"] = {
            platform: list(links)[:5] for platform, links in all_social.items()
        }

        # ìœ„í˜‘ ì •ë³´ (ìˆëŠ” ê²½ìš°)
        if deep_request.check_threats:
            threats = []
            for data in analyzed_urls.values():
                if "threat_info" in data:
                    if data["threat_info"]["threat_level"] not in ["None", "Low"]:
                        threats.append(
                            {
                                "url": data["url"],
                                "threat_level": data["threat_info"]["threat_level"],
                                "detected_by": data["threat_info"]["detected_by"],
                            }
                        )
            summary["threats_found"] = len(threats)
            summary["threats"] = threats

        # URL ê´€ê³„ í†µê³„
        summary["total_relationships"] = len(relationships)

        # ê¹Šì´ë³„ ë¶„í¬
        depth_distribution = {}
        for data in analyzed_urls.values():
            depth = data["depth"]
            depth_distribution[depth] = depth_distribution.get(depth, 0) + 1
        summary["depth_distribution"] = depth_distribution

        return summary


class VTClient:
    """VirusTotal ìœ„í˜‘ ì •ë³´ ì¡°íšŒ í´ë˜ìŠ¤"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.debug_mode = DEBUG_MODE

    def query_domain(self, domain: str) -> Dict[str, Any]:
        """ë„ë©”ì¸ í‰íŒ ì¡°íšŒ"""
        if self.debug_mode:
            return self._mock_domain_response(domain)

        if not self.api_key:
            return {
                "status": "error",
                "error": {"code": -32001, "message": "VirusTotal API KEY ì—†ìŒ"},
            }

        try:
            headers = {"x-apikey": self.api_key}
            url = f"https://www.virustotal.com/api/v3/domains/{domain}"

            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 404:
                return {
                    "status": "error",
                    "error": {
                        "code": -32003,
                        "message": f"ë„ë©”ì¸ '{domain}'ì„(ë¥¼) VirusTotal ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    },
                }
            elif response.status_code == 429:
                return {
                    "status": "error",
                    "error": {
                        "code": -32002,
                        "message": "VirusTotal ìš”ì²­ ì œí•œ ì´ˆê³¼ - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”",
                        "data": {"retry_after": 45},
                    },
                }
            elif response.status_code != 200:
                return {
                    "status": "error",
                    "error": {
                        "code": -32000,
                        "message": f"VirusTotal ì˜¤ë¥˜: {response.status_code}",
                    },
                }

            data = response.json()
            attributes = data.get("data", {}).get("attributes", {})

            last_analysis_stats = attributes.get(
                "last_analysis_stats",
                {"malicious": 0, "suspicious": 0, "undetected": 0},
            )

            threat_level = self._calculate_threat_level(last_analysis_stats)

            return {
                "status": "success",
                "data": {
                    "domain": domain,
                    "threat_level": threat_level,
                    "detected_by": last_analysis_stats.get("malicious", 0),
                    "analysis_stats": last_analysis_stats,
                    "last_analysis_date": attributes.get("last_analysis_date"),
                    "categories": attributes.get("categories", {}),
                },
                "metadata": {
                    "source": "virustotal",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "query_time_ms": 0,
                },
            }
        except requests.exceptions.Timeout:
            return {
                "status": "error",
                "error": {"code": -32005, "message": "VirusTotal ìš”ì²­ íƒ€ì„ì•„ì›ƒ"},
            }
        except Exception as e:
            return {
                "status": "error",
                "error": {"code": -32000, "message": f"VirusTotal ì˜¤ë¥˜: {str(e)}"},
            }

    def query_ip(self, ip_address: str) -> Dict[str, Any]:
        """IP ì£¼ì†Œ í‰íŒ ì¡°íšŒ"""
        if self.debug_mode:
            return self._mock_ip_response(ip_address)

        if not self.api_key:
            return {
                "status": "error",
                "error": {"code": -32001, "message": "VirusTotal API KEY ì—†ìŒ"},
            }

        try:
            headers = {"x-apikey": self.api_key}
            url = f"https://www.virustotal.com/api/v3/ip_addresses/{ip_address}"

            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 404:
                return {
                    "status": "error",
                    "error": {
                        "code": -32003,
                        "message": f"IP '{ip_address}'ì„(ë¥¼) VirusTotal ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    },
                }
            elif response.status_code == 429:
                return {
                    "status": "error",
                    "error": {
                        "code": -32002,
                        "message": "VirusTotal ìš”ì²­ ì œí•œ ì´ˆê³¼ - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”",
                        "data": {"retry_after": 45},
                    },
                }
            elif response.status_code != 200:
                return {
                    "status": "error",
                    "error": {
                        "code": -32000,
                        "message": f"VirusTotal ì˜¤ë¥˜: {response.status_code}",
                    },
                }

            data = response.json()
            attributes = data.get("data", {}).get("attributes", {})

            last_analysis_stats = attributes.get(
                "last_analysis_stats",
                {"malicious": 0, "suspicious": 0, "undetected": 0},
            )

            threat_level = self._calculate_threat_level(last_analysis_stats)

            return {
                "status": "success",
                "data": {
                    "ip_address": ip_address,
                    "country": attributes.get("country", "Unknown"),
                    "asn": attributes.get("asn"),
                    "organization": attributes.get("as_owner", "Unknown"),
                    "threat_level": threat_level,
                    "detected_by": last_analysis_stats.get("malicious", 0),
                    "analysis_stats": last_analysis_stats,
                },
                "metadata": {
                    "source": "virustotal",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "query_time_ms": 0,
                },
            }
        except requests.exceptions.Timeout:
            return {
                "status": "error",
                "error": {"code": -32005, "message": "VirusTotal ìš”ì²­ íƒ€ì„ì•„ì›ƒ"},
            }
        except Exception as e:
            logger.error(f"VirusTotal IP ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return {
                "status": "error",
                "error": {"code": -32000, "message": f"VirusTotal ì˜¤ë¥˜: {str(e)}"},
            }

    def _calculate_threat_level(self, stats: Dict[str, int]) -> str:
        """ìœ„í˜‘ ìˆ˜ì¤€ ê³„ì‚°"""
        malicious = stats.get("malicious", 0)
        suspicious = stats.get("suspicious", 0)

        if malicious >= 5:
            return "Critical"
        elif malicious >= 2:
            return "High"
        elif malicious + suspicious >= 5:
            return "Medium"
        elif malicious + suspicious > 0:
            return "Low"
        else:
            return "None"

    def _mock_domain_response(self, domain: str) -> Dict[str, Any]:
        """Mock ë„ë©”ì¸ ì‘ë‹µ"""
        return {
            "status": "success",
            "data": {
                "domain": domain,
                "threat_level": "Low",
                "detected_by": 0,
                "analysis_stats": {"malicious": 0, "suspicious": 0, "undetected": 89},
                "last_analysis_date": datetime.now(timezone.utc).isoformat(),
                "categories": {"Sophos": "legitimate", "Kaspersky": "legitimate"},
            },
            "metadata": {
                "source": "virustotal",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "query_time_ms": 245,
                "note": "Mock data in DEBUG_MODE",
            },
        }

    def _mock_ip_response(self, ip: str) -> Dict[str, Any]:
        """Mock IP ì‘ë‹µ"""
        return {
            "status": "success",
            "data": {
                "ip_address": ip,
                "country": "US",
                "asn": 15169,
                "organization": "Google LLC",
                "threat_level": "None",
                "detected_by": 0,
                "analysis_stats": {"malicious": 0, "suspicious": 0, "undetected": 89},
            },
            "metadata": {
                "source": "virustotal",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "query_time_ms": 198,
                "note": "Mock data in DEBUG_MODE",
            },
        }


# ============================================================================
# Phase 3: Client ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# ============================================================================

intelx_client = IntelligenceXClient(INTELX_API_KEY)
sherlock_client = SherlockClient()
playwright_client = PlaywrightClient()
vt_client = VTClient(VIRUSTOTAL_API_KEY)

# ============================================================================
# Phase 4: MCP ì„œë²„ ì´ˆê¸°í™”
# ============================================================================

server = FastMCP("osint-mcp-server")


# ============================================================================
# Phase 5: MCP Tools (@server.tool() ë°ì½”ë ˆì´í„° ì‚¬ìš©)
# ============================================================================


@server.tool()
def search_intelligence_x(request: SearchRequest) -> str:
    """
    Intelligence Xì—ì„œ ë‹¤í¬ì›¹ ë° ìœ ì¶œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        request: ê²€ìƒ‰ ìš”ì²­ ì •ë³´

    Returns:
        JSON í˜•ì‹ì˜ ê²€ìƒ‰ ê²°ê³¼
    """
    try:
        start_time = time.time()
        result = intelx_client.search(request)
        execution_time = (time.time() - start_time) * 1000

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"Intelligence X ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
def search_username_sherlock(request: SherlockSearchRequest) -> str:
    """
    Sherlockì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìëª…ì„ ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        request: Sherlock ê²€ìƒ‰ ìš”ì²­ ì •ë³´

    Returns:
        JSON í˜•ì‹ì˜ ê²€ìƒ‰ ê²°ê³¼
    """
    try:
        start_time = time.time()
        result = sherlock_client.search(request)
        execution_time = (time.time() - start_time) * 1000

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"Sherlock ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
async def analyze_url_playwright(request: PlaywrightAnalyzeRequest) -> str:
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ URLì„ ë¶„ì„í•©ë‹ˆë‹¤.

    Args:
        request: Playwright ë¶„ì„ ìš”ì²­ ì •ë³´

    Returns:
        JSON í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼
    """
    try:
        start_time = time.time()
        result = await playwright_client.analyze(request)
        execution_time = (time.time() - start_time) * 1000

        # PDF ìƒì„± (ë¹„ë™ê¸°)
        pdf_path = ""
        try:
            pdf_path = await pdf_generator.url_to_pdf(request.url)
        except Exception as pdf_error:
            logger.warning(f"PDF ìƒì„± ì‹¤íŒ¨: {pdf_error}")

        # ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±
        summary = f"URL: {request.url}"
        if "metadata" in result and "title" in result["metadata"]:
            summary += f" | ì œëª©: {result['metadata']['title']}"

        # ì¤‘ìš” ì •ë³´ ì¶”ì¶œ
        sensitive_info = {}
        if "entities" in result:
            entities = result["entities"]
            if "emails" in entities:
                sensitive_info["emails"] = entities["emails"]
            if "phones" in entities:
                sensitive_info["phones"] = entities["phones"]
            if "social_media" in entities:
                sensitive_info["social_media"] = entities["social_media"]

        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        try:
            osint_db.add_record(
                target=request.url,
                url=request.url,
                pdf_path=pdf_path,
                summary=summary,
                sensitive_info=sensitive_info,
                collection_method="analyze_url_playwright",
                threat_level="unknown",
                metadata=result
            )
        except Exception as db_error:
            logger.warning(f"DB ì €ì¥ ì‹¤íŒ¨: {db_error}")

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
                "saved_to_db": True,
                "pdf_path": pdf_path
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"URL ë¶„ì„ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
def check_virustotal_domain(request: ThreatIntelRequest) -> str:
    """
    VirusTotalì—ì„œ ë„ë©”ì¸ì˜ ìœ„í˜‘ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

    Args:
        request: ìœ„í˜‘ ì •ë³´ ì¡°íšŒ ìš”ì²­ (query_type: 'domain' ì‚¬ìš©)

    Returns:
        JSON í˜•ì‹ì˜ ìœ„í˜‘ ì •ë³´
    """
    try:
        start_time = time.time()
        result = vt_client.query_domain(request.query)
        execution_time = (time.time() - start_time) * 1000

        # ìœ„í˜‘ ìˆ˜ì¤€ ê²°ì •
        threat_level = "safe"
        if "stats" in result:
            stats = result["stats"]
            malicious = stats.get("malicious", 0)
            suspicious = stats.get("suspicious", 0)

            if malicious > 0:
                threat_level = "malicious"
            elif suspicious > 0:
                threat_level = "suspicious"

        # ìš”ì•½ ìƒì„±
        summary = f"VirusTotal ë„ë©”ì¸ ì¡°íšŒ: {request.query}"
        if "stats" in result:
            summary += f" | ì•…ì„±: {result['stats'].get('malicious', 0)}, ì˜ì‹¬: {result['stats'].get('suspicious', 0)}"

        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        try:
            osint_db.add_record(
                target=request.query,
                url=f"https://{request.query}",
                pdf_path="",
                summary=summary,
                sensitive_info={},
                collection_method="check_virustotal_domain",
                threat_level=threat_level,
                metadata=result
            )
        except Exception as db_error:
            logger.warning(f"DB ì €ì¥ ì‹¤íŒ¨: {db_error}")

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
                "saved_to_db": True
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"VirusTotal ë„ë©”ì¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
def check_virustotal_ip(request: ThreatIntelRequest) -> str:
    """
    VirusTotalì—ì„œ IP ì£¼ì†Œì˜ ìœ„í˜‘ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

    Args:
        request: ìœ„í˜‘ ì •ë³´ ì¡°íšŒ ìš”ì²­ (query_type: 'ip' ì‚¬ìš©)

    Returns:
        JSON í˜•ì‹ì˜ ìœ„í˜‘ ì •ë³´
    """
    try:
        start_time = time.time()
        result = vt_client.query_ip(request.query)
        execution_time = (time.time() - start_time) * 1000

        # ìœ„í˜‘ ìˆ˜ì¤€ ê²°ì •
        threat_level = "safe"
        if "stats" in result:
            stats = result["stats"]
            malicious = stats.get("malicious", 0)
            suspicious = stats.get("suspicious", 0)

            if malicious > 0:
                threat_level = "malicious"
            elif suspicious > 0:
                threat_level = "suspicious"

        # ìš”ì•½ ìƒì„±
        summary = f"VirusTotal IP ì¡°íšŒ: {request.query}"
        if "stats" in result:
            summary += f" | ì•…ì„±: {result['stats'].get('malicious', 0)}, ì˜ì‹¬: {result['stats'].get('suspicious', 0)}"

        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        try:
            osint_db.add_record(
                target=request.query,
                url=f"http://{request.query}",
                pdf_path="",
                summary=summary,
                sensitive_info={},
                collection_method="check_virustotal_ip",
                threat_level=threat_level,
                metadata=result
            )
        except Exception as db_error:
            logger.warning(f"DB ì €ì¥ ì‹¤íŒ¨: {db_error}")

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
                "saved_to_db": True
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"VirusTotal IP ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
async def crawl_and_analyze_url(request: PlaywrightCrawlRequest) -> str:
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ URLì„ ìë™ìœ¼ë¡œ í¬ë¡¤ë§í•˜ê³  ì§€ëŠ¥í˜• ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        request: Playwright í¬ë¡¤ë§ ìš”ì²­ ì •ë³´

    Returns:
        JSON í˜•ì‹ì˜ í¬ë¡¤ë§ ê²°ê³¼
    """
    try:
        start_time = time.time()
        result = await playwright_client.crawl(request)
        execution_time = (time.time() - start_time) * 1000

        # PDF ìƒì„± (ì²« ë²ˆì§¸ í˜ì´ì§€ë§Œ)
        pdf_path = ""
        try:
            pdf_path = await pdf_generator.url_to_pdf(request.url)
        except Exception as pdf_error:
            logger.warning(f"PDF ìƒì„± ì‹¤íŒ¨: {pdf_error}")

        # í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½
        summary = f"URL í¬ë¡¤ë§: {request.url}"
        if "summary" in result:
            crawl_summary = result["summary"]
            summary += f" | ë°©ë¬¸ í˜ì´ì§€: {crawl_summary.get('total_pages', 0)}ê°œ"

        # ëª¨ë“  í˜ì´ì§€ì—ì„œ ì¤‘ìš” ì •ë³´ ìˆ˜ì§‘
        all_emails = []
        all_phones = []
        all_social = []

        if "pages" in result:
            for page in result["pages"]:
                if "entities" in page:
                    entities = page["entities"]
                    all_emails.extend(entities.get("emails", []))
                    all_phones.extend(entities.get("phones", []))
                    all_social.extend(entities.get("social_media", []))

        # ì¤‘ë³µ ì œê±°
        sensitive_info = {
            "emails": list(set(all_emails)),
            "phones": list(set(all_phones)),
            "social_media": list(set(all_social))
        }

        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        try:
            osint_db.add_record(
                target=request.url,
                url=request.url,
                pdf_path=pdf_path,
                summary=summary,
                sensitive_info=sensitive_info,
                collection_method="crawl_and_analyze_url",
                threat_level="unknown",
                metadata=result
            )
        except Exception as db_error:
            logger.warning(f"DB ì €ì¥ ì‹¤íŒ¨: {db_error}")

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
                "saved_to_db": True,
                "pdf_path": pdf_path
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
async def interact_with_webpage(request: PlaywrightInteractionRequest) -> str:
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹í˜ì´ì§€ì™€ ë™ì ìœ¼ë¡œ ìƒí˜¸ì‘ìš©í•©ë‹ˆë‹¤.

    ë²„íŠ¼ í´ë¦­, í¼ ì…ë ¥, ìŠ¤í¬ë¡¤, ë„¤ë¹„ê²Œì´ì…˜ ë“± ì‚¬ìš©ìì²˜ëŸ¼ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì¡°ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì•¡ì…˜ ì‹œí€€ìŠ¤ë¥¼ ì •ì˜í•˜ì—¬ ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ì§€ì›ë˜ëŠ” ì•¡ì…˜:
    - click: ìš”ì†Œ í´ë¦­
    - type/fill: í…ìŠ¤íŠ¸ ì…ë ¥
    - press: í‚¤ ì…ë ¥ (Enter, Tab ë“±)
    - select: ë“œë¡­ë‹¤ìš´ ì„ íƒ
    - scroll: ìŠ¤í¬ë¡¤
    - wait: ëŒ€ê¸°
    - wait_for_selector: ìš”ì†Œ ë¡œë“œ ëŒ€ê¸°
    - screenshot: ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜
    - navigate: í˜ì´ì§€ ì´ë™
    - hover: ë§ˆìš°ìŠ¤ ì˜¤ë²„
    - check/uncheck: ì²´í¬ë°•ìŠ¤
    - get_text: í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    - get_attribute: ì†ì„± ê°€ì ¸ì˜¤ê¸°

    Args:
        request: Playwright ìƒí˜¸ì‘ìš© ìš”ì²­ ì •ë³´

    Returns:
        JSON í˜•ì‹ì˜ ì‹¤í–‰ ê²°ê³¼ (ê° ì•¡ì…˜ì˜ ì„±ê³µ/ì‹¤íŒ¨, ìµœì¢… ë°ì´í„°, ìŠ¤í¬ë¦°ìƒ· ë“±)
    """
    try:
        start_time = time.time()
        result = await playwright_client.interact(request)
        execution_time = (time.time() - start_time) * 1000

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"ìƒí˜¸ì‘ìš© ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
async def auto_explore_webpage(request: PlaywrightAutoExploreRequest) -> str:
    """
    ëª©í‘œ ê¸°ë°˜ ìë™ ì›¹ íƒìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Agentê°€ ëª©í‘œë¥¼ ì´í•´í•˜ê³  ìë™ìœ¼ë¡œ ê´€ë ¨ ë§í¬ë¥¼ ì°¾ì•„ í´ë¦­í•˜ë©° ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ í˜ì´ì§€ë¥¼ ë¶„ì„í•˜ê³  ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ìš”ì†Œì™€ ìƒí˜¸ì‘ìš©í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ ëª©í‘œ:
    - "ì—°ë½ì²˜ ì •ë³´ ì°¾ê¸°"
    - "SNS ë§í¬ ì°¾ê¸°"
    - "ì´ë©”ì¼ ì£¼ì†Œ ìˆ˜ì§‘"
    - "íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ í˜ì´ì§€ ì°¾ê¸°"

    Args:
        request: ìë™ íƒìƒ‰ ìš”ì²­ ì •ë³´ (URL, ëª©í‘œ, ì œí•œì‚¬í•­)

    Returns:
        JSON í˜•ì‹ì˜ íƒìƒ‰ ê²°ê³¼ (ë°©ë¬¸í•œ í˜ì´ì§€, ìƒí˜¸ì‘ìš© ë‚´ì—­, ë°œê²¬ì‚¬í•­)
    """
    try:
        start_time = time.time()
        result = await playwright_client.auto_explore(request)
        execution_time = (time.time() - start_time) * 1000

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"ìë™ íƒìƒ‰ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
async def deep_analyze_urls(request: PlaywrightDeepAnalyzeRequest) -> str:
    """
    ì¬ê·€ì  URL ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    ì œê³µëœ URLì„ ë¶„ì„í•˜ê³ , ê·¸ í˜ì´ì§€ì—ì„œ ë°œê²¬ëœ ëª¨ë“  URLë“¤ë„ ìë™ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    ì™¸ë¶€ ë„ë©”ì¸ í¬í•¨ ì—¬ë¶€, ë¶„ì„ ê¹Šì´, ìµœëŒ€ URL ìˆ˜ ë“±ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ê° URLì— ëŒ€í•´:
    - ë©”íƒ€ë°ì´í„° (ì œëª©, ì„¤ëª…)
    - ì´ë©”ì¼ ì£¼ì†Œ ì¶”ì¶œ
    - ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
    - ì†Œì…œ ë¯¸ë””ì–´ ë§í¬ ì¶”ì¶œ
    - ìœ„í˜‘ ì •ë³´ (ì„ íƒì , VirusTotal)
    - URL ê°„ ê´€ê³„ ë§µí•‘

    ì‚¬ìš© ì‚¬ë¡€:
    - íŠ¹ì • ì›¹ì‚¬ì´íŠ¸ì˜ ì „ì²´ êµ¬ì¡° íŒŒì•…
    - ì›¹ì‚¬ì´íŠ¸ì—ì„œ ëª¨ë“  ì—°ë½ì²˜ ì •ë³´ ìˆ˜ì§‘
    - ë§í¬ëœ ëª¨ë“  ì™¸ë¶€ ë¦¬ì†ŒìŠ¤ í™•ì¸
    - ì›¹ì‚¬ì´íŠ¸ ë³´ì•ˆ ë¶„ì„ (ëª¨ë“  ë§í¬ëœ ë„ë©”ì¸ ìœ„í˜‘ ê²€ì‚¬)

    Args:
        request: ì¬ê·€ì  ë¶„ì„ ìš”ì²­ ì •ë³´

    Returns:
        JSON í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼:
        - urls: ê° URLì˜ ìƒì„¸ ì •ë³´
        - relationships: URL ê°„ ë¶€ëª¨-ìì‹ ê´€ê³„
        - summary: í†µí•© ìš”ì•½ (ì´ë©”ì¼, ì „í™”ë²ˆí˜¸, SNS, ìœ„í˜‘ ì •ë³´ ë“±)
    """
    try:
        start_time = time.time()
        result = await playwright_client.deep_analyze(request)
        execution_time = (time.time() - start_time) * 1000

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"ì¬ê·€ì  URL ë¶„ì„ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


if __name__ == "__main__":
    logger.info("=" * 70)
    logger.info("Starting OSINT Unified MCP Server (fastmcp - STDIO mode)")
    logger.info("=" * 70)
    logger.info(f"DEBUG ëª¨ë“œ: {DEBUG_MODE}")
    logger.info("Transport: STDIO (Claude Desktop í˜¸í™˜)")
    logger.info("")
    logger.info("âœ… í™œì„±í™”ëœ OSINT ë„êµ¬:")
    logger.info("   1. search_intelligence_x - ë‹¤í¬ì›¹/ìœ ì¶œ ë°ì´í„° ê²€ìƒ‰")
    logger.info("   2. search_username_sherlock - ì‚¬ìš©ìëª… ê²€ìƒ‰")
    logger.info("   3. analyze_url_playwright - URL ë¶„ì„")
    logger.info("   4. check_virustotal_domain - VirusTotal ë„ë©”ì¸ í™•ì¸")
    logger.info("   5. check_virustotal_ip - VirusTotal IP í™•ì¸")
    logger.info("   6. crawl_and_analyze_url - URL ìë™ í¬ë¡¤ë§ & ì§€ëŠ¥í˜• ë¶„ì„")
    logger.info("")
    logger.info("ğŸš€ ë™ì  ìƒí˜¸ì‘ìš© ë„êµ¬:")
    logger.info("   7. interact_with_webpage - ì›¹í˜ì´ì§€ ë™ì  ìƒí˜¸ì‘ìš©")
    logger.info("      (í´ë¦­, ì…ë ¥, ìŠ¤í¬ë¡¤, ë„¤ë¹„ê²Œì´ì…˜ ë“±)")
    logger.info("   8. auto_explore_webpage - ëª©í‘œ ê¸°ë°˜ ìë™ íƒìƒ‰")
    logger.info("      (Agentê°€ ìë™ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ íƒìƒ‰)")
    logger.info("")
    logger.info("ğŸ” ì¬ê·€ì  URL ë¶„ì„:")
    logger.info("   9. deep_analyze_urls - ì¬ê·€ì  URL ë¶„ì„ & ê´€ê³„ ë§¤í•‘")
    logger.info("      (URLì„ ë¶„ì„í•˜ê³  ë°œê²¬ëœ ëª¨ë“  URLë„ ìë™ ë¶„ì„)")
    logger.info("=" * 70)

    # STDIO ëª¨ë“œë¡œ ëª…ì‹œì  ì‹¤í–‰
    server.run(transport="stdio")
