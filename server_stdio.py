#!/usr/bin/env python3
"""
OSINT Unified MCP Server - fastmcp Architecture (STDIO Mode)
í†µí•© OSINT MCP ì„œë²„ - fastmcp ê¸°ë°˜ (stdio ê¸°ë°˜)

Phase 1 êµ¬í˜„: ê¸°ì¡´ server.pyì˜ 7ê°œ toolì„ fastmcpë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
ëª©í‘œ: fastmcpë¥¼ í†µí•œ stdio ê¸°ë°˜ MCP ì„œë²„ (Claude Desktop í˜¸í™˜)
"""

import os
import json
import time
import logging
import asyncio
import subprocess
import shutil
import base64
from typing import Any, Dict, List, Optional
from datetime import datetime, timezone

from dotenv import load_dotenv

try:
    from intelxapi import intelx
except ImportError:
    intelx = None
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import requests
from pydantic import BaseModel, Field

from fastmcp import FastMCP

# ============================================================================
# Phase 0: ì´ˆê¸°í™” ë° í™˜ê²½ì„¤ì •
# ============================================================================

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# API ì„¤ì •
INTELX_API_KEY = os.getenv("INTELX_API_KEY", "")
VIRUSTOTAL_API_KEY = os.getenv("VIRUSTOTAL_API_KEY", "")
SHODAN_API_KEY = os.getenv("SHODAN_API_KEY", "")
HARVESTER_API_KEY = os.getenv("HARVESTER_API_KEY", "")
DEBUG_MODE = os.getenv("DEBUG_MODE", "false").lower() == "true"

# API í‚¤ ìœ íš¨ì„± í™•ì¸
if not INTELX_API_KEY and not DEBUG_MODE:
    logger.warning("Intelligence X API KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

if not VIRUSTOTAL_API_KEY and not DEBUG_MODE:
    logger.warning(
        "VirusTotal API KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (DEBUG_MODE=trueì¸ ê²½ìš° Mock ë°ì´í„° ì‚¬ìš©)"
    )

if DEBUG_MODE:
    logger.info("ğŸ”§ DEBUG_MODE í™œì„±í™” - Mock ë°ì´í„° ì‚¬ìš©")

# ============================================================================
# Phase 1: Pydantic ëª¨ë¸
# ============================================================================


class SearchRequest(BaseModel):
    term: str = Field(..., description="ê²€ìƒ‰í•  ì…€ë ‰í„° (ì´ë©”ì¼, ë„ë©”ì¸, IP ë“±)")
    maxresults: int = Field(100, description="ìµœëŒ€ ê²°ê³¼ ìˆ˜")
    timeout: int = Field(5, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)")
    buckets: Optional[List[str]] = Field(None, description="ê²€ìƒ‰í•  ë²„í‚· ëª©ë¡")
    datefrom: Optional[str] = Field(None, description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    dateto: Optional[str] = Field(None, description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")


class SherlockSearchRequest(BaseModel):
    username: str = Field(..., description="ê²€ìƒ‰í•  ì‚¬ìš©ìëª…")
    sites: Optional[List[str]] = Field(
        None, description="ê²€ìƒ‰í•  ì‚¬ì´íŠ¸ ëª©ë¡ (ì˜ˆ: ['github', 'twitter'])"
    )
    timeout: int = Field(120, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 120ì´ˆ)")


class PlaywrightAnalyzeRequest(BaseModel):
    url: str = Field(..., description="ë¶„ì„í•  URL")
    extract_metadata: bool = Field(
        True, description="ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì œëª©, ì„¤ëª…, ì´ë¯¸ì§€)"
    )
    extract_text: bool = Field(True, description="í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    extract_links: bool = Field(True, description="ë§í¬ ëª©ë¡ ì¶”ì¶œ")
    screenshot: bool = Field(False, description="ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜")
    wait_for_selector: Optional[str] = Field(
        None, description="íŠ¹ì • ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°"
    )
    timeout: int = Field(30, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


class PlaywrightCrawlRequest(BaseModel):
    url: str = Field(..., description="ì‹œì‘í•  URL")
    max_depth: int = Field(2, description="í¬ë¡¤ë§ ê¹Šì´ (ê¸°ë³¸ê°’: 2)")
    max_pages: int = Field(10, description="ìµœëŒ€ ë°©ë¬¸ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 10)")
    url_pattern: Optional[str] = Field(
        None, description="í¬ë¡¤ë§í•  URL íŒ¨í„´ (ì •ê·œí‘œí˜„ì‹, ì˜ˆ: .*github.com.*)"
    )
    extract_text: bool = Field(True, description="í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    extract_links: bool = Field(True, description="ë§í¬ ì¶”ì¶œ")
    analyze_content: bool = Field(True, description="ì§€ëŠ¥í˜• ì½˜í…ì¸  ë¶„ì„")
    timeout: int = Field(60, description="ì „ì²´ í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


class ThreatIntelRequest(BaseModel):
    query: str = Field(..., description="ì¡°íšŒí•  ëŒ€ìƒ (ë„ë©”ì¸ ë˜ëŠ” IP)")
    query_type: str = Field("domain", description="ì¡°íšŒ íƒ€ì…: domain ë˜ëŠ” ip")
    timeout: int = Field(10, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


# ============================================================================
# Phase 2: Client Classes (ê¸°ì¡´ êµ¬í˜„ ìœ ì§€)
# ============================================================================


class IntelligenceXClient:
    """Intelligence X ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, api_key: str):
        self.client = None
        self.debug_mode = DEBUG_MODE

        if api_key and intelx is not None:
            self.client = intelx(api_key)
            self.client.API_ROOT = "https://free.intelx.io"

    def search(self, search_request: SearchRequest) -> Dict[str, Any]:
        if self.debug_mode:
            logger.info(f"DEBUG MODE: Mock ë°ì´í„° ë°˜í™˜ (ê²€ìƒ‰ì–´: {search_request.term})")
            return {
                "records": [
                    {
                        "name": f"Mock Result 1 for {search_request.term}",
                        "description": "This is a mock result for testing purposes",
                        "date": datetime.now().isoformat(),
                        "media": 1,
                        "type": 1,
                        "added": datetime.now().isoformat(),
                        "storageid": "mock-storage-id-1",
                        "bucket": "mock-bucket",
                    },
                    {
                        "name": f"Mock Result 2 for {search_request.term}",
                        "description": "Second mock result for testing",
                        "date": "2025-10-25T12:00:00",
                        "media": 1,
                        "type": 1,
                        "added": "2025-10-25T14:30:00",
                        "storageid": "mock-storage-id-2",
                        "bucket": "mock-bucket",
                    },
                ]
            }

        try:
            if not self.client:
                raise ValueError("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            results = self.client.search(
                search_request.term,
                maxresults=search_request.maxresults,
                timeout=search_request.timeout,
            )
            return results
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise ValueError(f"Intelligence X API ì˜¤ë¥˜: {str(e)}")


class SherlockClient:
    """Sherlock ì‚¬ìš©ìëª… ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.debug_mode = DEBUG_MODE

    def search(self, search_request: SherlockSearchRequest) -> Dict[str, Any]:
        """Sherlockìœ¼ë¡œ ì‚¬ìš©ìëª… ê²€ìƒ‰"""
        if self.debug_mode:
            logger.info(
                f"DEBUG MODE: Sherlock Mock ë°ì´í„° ë°˜í™˜ (ì‚¬ìš©ìëª…: {search_request.username})"
            )
            return {
                "found": {
                    "github": {
                        "url": f"https://github.com/{search_request.username}",
                        "status": "found",
                    },
                    "twitter": {
                        "url": f"https://twitter.com/{search_request.username}",
                        "status": "found",
                    },
                },
                "not_found": ["instagram", "reddit"],
                "total_found": 2,
                "total_checked": 4,
            }

        try:
            # Sherlock ì ˆëŒ€ ê²½ë¡œ ì°¾ê¸°
            sherlock_path = shutil.which("sherlock")
            if not sherlock_path:
                raise FileNotFoundError("Sherlockì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install sherlock-project' ì‹¤í–‰í•˜ì„¸ìš”.")

            cmd = [sherlock_path, search_request.username, "--no-color", "--no-txt"]

            if search_request.sites:
                for site in search_request.sites:
                    cmd.extend(["--site", site])

            logger.info(f"Sherlock ê²€ìƒ‰ ì‹¤í–‰: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=search_request.timeout * len(search_request.sites or [100]),
            )

            found_accounts = {}
            lines = result.stdout.split("\n")

            for line in lines:
                if line.strip().startswith("[+]"):
                    parts = line.strip()[4:].split(": ", 1)
                    if len(parts) == 2:
                        site_name = parts[0].strip()
                        url = parts[1].strip()
                        found_accounts[site_name] = {"url": url, "status": "found"}

            total_found = len(found_accounts)

            return {
                "found": found_accounts,
                "total_found": total_found,
                "username": search_request.username,
                "timestamp": datetime.now().isoformat(),
                "status": "completed",
            }
        except subprocess.TimeoutExpired:
            logger.error(f"Sherlock íƒ€ì„ì•„ì›ƒ: {search_request.username}")
            raise TimeoutError("Sherlock ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            logger.error(f"Sherlock ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise ValueError(f"Sherlock ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")


class PlaywrightClient:
    """Playwright URL ë¶„ì„ ë° ìë™ í¬ë¡¤ë§ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.debug_mode = DEBUG_MODE
        self.visited_urls = set()
        self.crawl_results = []

    async def analyze(
        self, analyze_request: PlaywrightAnalyzeRequest
    ) -> Dict[str, Any]:
        """Playwrightë¡œ URL ë¶„ì„"""
        if self.debug_mode:
            logger.info(
                f"DEBUG MODE: Playwright Mock ë°ì´í„° ë°˜í™˜ (URL: {analyze_request.url})"
            )
            return {
                "url": analyze_request.url,
                "metadata": {
                    "title": "Mock Page Title",
                    "description": "This is a mock page description",
                    "image": "https://example.com/image.jpg",
                },
                "text": "Mock page content...",
                "links": [
                    {"text": "Link 1", "href": "https://example.com/link1"},
                    {"text": "Link 2", "href": "https://example.com/link2"},
                ],
                "screenshot": None,
                "status": "completed",
            }

        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch()
                page = await browser.new_page()

                logger.info(f"Playwright í˜ì´ì§€ ë¡œë“œ: {analyze_request.url}")
                await page.goto(
                    analyze_request.url,
                    timeout=analyze_request.timeout * 1000,
                    wait_until="load",
                )

                if analyze_request.wait_for_selector:
                    await page.wait_for_selector(
                        analyze_request.wait_for_selector, timeout=5000
                    )

                result = {"url": analyze_request.url, "status": "completed"}

                if analyze_request.extract_metadata:
                    title = await page.title()

                    try:
                        meta_description = await page.locator(
                            'meta[name="description"]'
                        ).get_attribute("content", timeout=1000)
                    except:
                        meta_description = None

                    try:
                        meta_image = await page.locator(
                            'meta[property="og:image"]'
                        ).get_attribute("content", timeout=1000)
                    except:
                        meta_image = None

                    result["metadata"] = {
                        "title": title,
                        "description": meta_description or "",
                        "image": meta_image or "",
                    }

                if analyze_request.extract_text:
                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")
                    for script in soup(["script", "style"]):
                        script.decompose()
                    text = soup.get_text(separator="\n", strip=True)
                    result["text"] = text[:2000] if text else ""

                if analyze_request.extract_links:
                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")
                    links = []
                    for link in soup.find_all("a", href=True):
                        links.append(
                            {"text": link.get_text(strip=True), "href": link["href"]}
                        )
                    result["links"] = links[:50]

                if analyze_request.screenshot:
                    screenshot_bytes = await page.screenshot()
                    result["screenshot"] = base64.b64encode(screenshot_bytes).decode(
                        "utf-8"
                    )

                await browser.close()
                return result

        except Exception as e:
            logger.error(f"Playwright ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise ValueError(f"URL ë¶„ì„ ì˜¤ë¥˜: {str(e)}")

    def _analyze_content(self, html: str, text: str, url: str) -> Dict[str, Any]:
        """í˜ì´ì§€ ì½˜í…ì¸ ì˜ ì§€ëŠ¥í˜• ë¶„ì„"""
        import re

        analysis = {
            "page_purpose": self._detect_page_purpose(text, html, url),
            "key_information": self._extract_key_info(text, html),
            "potential_risks": self._detect_risks(text, html, url),
            "entities": self._extract_entities(text),
            "keywords": self._extract_keywords(text),
        }
        return analysis

    def _detect_page_purpose(self, text: str, html: str, url: str) -> str:
        """í˜ì´ì§€ì˜ ëª©ì  íŒŒì•…"""
        text_lower = text.lower()
        url_lower = url.lower()

        # í”„ë¡œí•„ í˜ì´ì§€ ê°ì§€
        if any(
            keyword in text_lower for keyword in ["profile", "about", "bio", "user"]
        ) or any(keyword in url_lower for keyword in ["profile", "user", "about"]):
            return "User/Profile Page"

        # ë¡œê·¸ì¸ í˜ì´ì§€
        if any(
            keyword in text_lower
            for keyword in ["login", "password", "username", "sign in"]
        ):
            return "Authentication/Login Page"

        # ìƒê±°ë˜ ì‚¬ì´íŠ¸
        if any(
            keyword in text_lower
            for keyword in ["price", "buy", "purchase", "cart", "checkout", "product"]
        ):
            return "E-commerce/Shopping Page"

        # ë¬¸ì„œ/ë¸”ë¡œê·¸
        if any(
            keyword in text_lower
            for keyword in ["article", "blog", "post", "author", "published"]
        ):
            return "Blog/Article Page"

        # ê²€ìƒ‰ ê²°ê³¼
        if any(
            keyword in text_lower for keyword in ["search result", "found", "matches"]
        ):
            return "Search Results Page"

        # API í˜ì´ì§€
        if "api" in url_lower or any(
            keyword in text_lower
            for keyword in ["endpoint", "request", "response", "json"]
        ):
            return "API/Technical Documentation"

        return "General Content Page"

    def _extract_key_info(self, text: str, html: str) -> Dict[str, Any]:
        """ì£¼ìš” ì •ë³´ ì¶”ì¶œ"""
        import re

        info = {}

        # ì´ë©”ì¼ ì¶”ì¶œ
        emails = set(
            re.findall(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", text)
        )
        if emails:
            info["emails"] = list(emails)[:5]  # ìµœëŒ€ 5ê°œ

        # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
        phones = set(re.findall(r"\b\d{3}[-.]?\d{3}[-.]?\d{4}\b", text))
        if phones:
            info["phone_numbers"] = list(phones)[:5]

        # ì†Œì…œ ë¯¸ë””ì–´ ë§í¬ ì¶”ì¶œ
        social_patterns = {
            "twitter": r"twitter\.com/[\w]+",
            "github": r"github\.com/[\w-]+",
            "linkedin": r"linkedin\.com/in/[\w-]+",
            "instagram": r"instagram\.com/[\w.]+",
            "facebook": r"facebook\.com/[\w.]+",
        }

        social_links = {}
        for platform, pattern in social_patterns.items():
            matches = re.findall(pattern, text)
            if matches:
                social_links[platform] = list(set(matches))[:3]

        if social_links:
            info["social_media"] = social_links

        # HTML ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´
        soup = BeautifulSoup(html, "html.parser")

        # Open Graph ì •ë³´
        og_title = soup.find("meta", property="og:title")
        og_description = soup.find("meta", property="og:description")

        if og_title:
            info["og_title"] = og_title.get("content")
        if og_description:
            info["og_description"] = og_description.get("content")

        # ì¡°ì§ëª… ì¶”ì¶œ ì‹œë„
        author = soup.find("meta", {"name": "author"})
        if author:
            info["author"] = author.get("content")

        return info

    def _detect_risks(self, text: str, html: str, url: str) -> List[str]:
        """ì ì¬ì  ë³´ì•ˆ ìœ„í—˜ ê°ì§€"""
        risks = []
        text_lower = text.lower()
        url_lower = url.lower()

        # í”¼ì‹± ì§•í›„
        if any(
            keyword in text_lower
            for keyword in [
                "verify account",
                "confirm identity",
                "update payment",
                "urgent action required",
            ]
        ):
            risks.append("Potential phishing indicators")

        # ì¸ì¦ ìš”êµ¬
        if "password" in text_lower or "login" in text_lower:
            risks.append("Authentication required - verify legitimacy")

        # ì˜ì‹¬ í™œë™
        if any(
            keyword in text_lower
            for keyword in [
                "limited time",
                "act now",
                "verify immediately",
                "confirm now",
            ]
        ):
            risks.append("High-pressure/urgency language detected")

        # ì™¸ë¶€ ìŠ¤í¬ë¦½íŠ¸ (XSS ê°€ëŠ¥ì„±)
        if "<script" in html and "src=" in html:
            script_count = html.count("<script")
            if script_count > 5:
                risks.append(f"Multiple external scripts detected ({script_count})")

        # HTTP vs HTTPS
        if url_lower.startswith("http://") and any(
            keyword in text_lower for keyword in ["password", "payment", "card"]
        ):
            risks.append("Sensitive data handling over insecure HTTP")

        # ìˆ¨ê²¨ì§„ í¼
        if "<form" in html and "style" in html.lower():
            if "display:none" in html.lower() or "visibility:hidden" in html.lower():
                risks.append("Hidden form elements detected")

        return risks if risks else ["No obvious security risks detected"]

    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜)"""
        import re

        entities = {}

        # URL ì¶”ì¶œ
        urls = re.findall(r'https?://[^\s<>"{}|\\^`\[\]]*', text)
        if urls:
            entities["urls"] = list(set(urls))[:10]

        # IP ì£¼ì†Œ ì¶”ì¶œ
        ips = re.findall(r"\b(?:\d{1,3}\.){3}\d{1,3}\b", text)
        if ips:
            entities["ip_addresses"] = list(set(ips))[:10]

        # ë„ë©”ì¸ ì¶”ì¶œ
        domains = re.findall(
            r"\b(?:[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?\.)+[a-z]{2,}\b", text.lower()
        )
        if domains:
            entities["domains"] = list(set(domains))[:10]

        return entities

    def _extract_keywords(self, text: str) -> List[str]:
        """ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ìˆœ ë¹ˆë„ ê¸°ë°˜)"""
        # ë¶ˆìš©ì–´ ì œì™¸
        stopwords = {
            "the",
            "a",
            "an",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "is",
            "was",
            "are",
            "be",
            "have",
            "has",
            "do",
            "does",
            "did",
            "this",
            "that",
            "these",
            "those",
            "i",
            "you",
            "he",
            "she",
            "it",
            "we",
            "they",
        }

        words = text.lower().split()
        # ê¸¸ì´ 4 ì´ìƒì˜ ë‹¨ì–´ë§Œ ê³ ë ¤
        filtered_words = [
            w for w in words if len(w) > 4 and w not in stopwords and w.isalpha()
        ]

        # ë¹ˆë„ ê³„ì‚°
        from collections import Counter

        word_freq = Counter(filtered_words)

        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
        keywords = [word for word, _ in word_freq.most_common(10)]
        return keywords

    async def crawl(self, crawl_request: PlaywrightCrawlRequest) -> Dict[str, Any]:
        """ìë™ í¬ë¡¤ë§ ë° ì§€ëŠ¥í˜• ë¶„ì„"""
        import re
        from urllib.parse import urljoin, urlparse

        if self.debug_mode:
            logger.info(
                f"DEBUG MODE: Crawler Mock ë°ì´í„° ë°˜í™˜ (URL: {crawl_request.url})"
            )
            return {
                "start_url": crawl_request.url,
                "pages_crawled": 3,
                "max_depth": crawl_request.max_depth,
                "summary": {
                    "primary_purpose": "Mock User Profile Page",
                    "key_findings": [
                        "4 repositories",
                        "Public profile",
                        "Active developer",
                    ],
                    "social_links": {"github": ["https://github.com/kjmkjmkj"]},
                    "risks": ["No obvious security risks detected"],
                },
                "pages": [
                    {
                        "url": crawl_request.url,
                        "title": "Mock Profile",
                        "purpose": "User/Profile Page",
                        "key_info": {
                            "social_media": {"github": ["https://github.com/kjmkjmkj"]}
                        },
                        "depth": 0,
                    }
                ],
                "status": "completed",
                "note": "Mock data in DEBUG_MODE",
            }

        try:
            self.visited_urls = set()
            self.crawl_results = []

            start_time = time.time()
            async with async_playwright() as p:
                browser = await p.chromium.launch()

                await self._crawl_recursive(
                    crawl_request.url, browser, crawl_request, depth=0
                )

                await browser.close()

            execution_time = time.time() - start_time

            # ë¶„ì„ ê²°ê³¼ ì¢…í•©
            summary = self._generate_summary(self.crawl_results, crawl_request.url)

            return {
                "start_url": crawl_request.url,
                "pages_crawled": len(self.crawl_results),
                "max_depth": crawl_request.max_depth,
                "summary": summary,
                "pages": self.crawl_results,
                "status": "completed",
                "execution_time_ms": int(execution_time * 1000),
            }

        except Exception as e:
            logger.error(f"Playwright í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise ValueError(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")

    async def _crawl_recursive(
        self, url: str, browser, crawl_request: PlaywrightCrawlRequest, depth: int
    ) -> None:
        """ì¬ê·€ì  í¬ë¡¤ë§"""
        import re
        from urllib.parse import urljoin, urlparse

        # ë°©ë¬¸ ì œí•œ í™•ì¸
        if url in self.visited_urls:
            return
        if len(self.crawl_results) >= crawl_request.max_pages:
            return
        if depth > crawl_request.max_depth:
            return

        # URL íŒ¨í„´ í™•ì¸
        if crawl_request.url_pattern:
            if not re.search(crawl_request.url_pattern, url):
                return

        # ê°™ì€ ë„ë©”ì¸ í™•ì¸
        start_domain = urlparse(crawl_request.url).netloc
        current_domain = urlparse(url).netloc
        if start_domain != current_domain:
            return

        self.visited_urls.add(url)

        try:
            page = await browser.new_page()
            logger.info(f"í¬ë¡¤ë§: {url} (ê¹Šì´: {depth})")

            await page.goto(
                url, timeout=crawl_request.timeout * 1000, wait_until="load"
            )

            # í˜ì´ì§€ ë¶„ì„
            html = await page.content()
            soup = BeautifulSoup(html, "html.parser")

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text(separator="\n", strip=True)

            # ë©”íƒ€ë°ì´í„°
            title = await page.title()

            # ë¶„ì„
            page_analysis = {}
            if crawl_request.analyze_content:
                page_analysis = self._analyze_content(html, text, url)

            # ë§í¬ ì¶”ì¶œ
            links = []
            if crawl_request.extract_links:
                for link in soup.find_all("a", href=True):
                    href = link["href"]
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    absolute_url = urljoin(url, href)
                    # í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±°
                    absolute_url = absolute_url.split("#")[0]
                    if absolute_url not in self.visited_urls:
                        links.append(absolute_url)

            # ê²°ê³¼ ì €ì¥
            self.crawl_results.append(
                {
                    "url": url,
                    "depth": depth,
                    "title": title,
                    "text": text[:1500] if crawl_request.extract_text else "",
                    **page_analysis,
                }
            )

            # ë‹¤ìŒ ë ˆë²¨ í¬ë¡¤ë§
            if depth < crawl_request.max_depth:
                for link in links[:5]:  # ê° í˜ì´ì§€ë‹¹ ìµœëŒ€ 5ê°œ ë§í¬ë§Œ ë”°ë¼ê°€ê¸°
                    if len(self.crawl_results) < crawl_request.max_pages:
                        await self._crawl_recursive(
                            link, browser, crawl_request, depth + 1
                        )

            await page.close()

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ({url}): {e}")

    def _generate_summary(self, results: List[Dict], start_url: str) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ê²°ê³¼ ì¢…í•© ë¶„ì„"""
        if not results:
            return {"status": "no_results"}

        # ì²« ë²ˆì§¸ í˜ì´ì§€ë¥¼ ì£¼ìš” ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ
        primary = results[0]

        summary = {
            "primary_purpose": primary.get("page_purpose", "Unknown"),
            "total_pages_analyzed": len(results),
            "key_findings": [],
            "all_entities": {
                "emails": set(),
                "social_media": {},
                "urls": set(),
                "domains": set(),
            },
            "risks": set(),
            "top_keywords": [],
        }

        # ëª¨ë“  í˜ì´ì§€ì—ì„œ ì •ë³´ í†µí•©
        for result in results:
            # ë¦¬ìŠ¤í¬ í†µí•©
            if "potential_risks" in result:
                summary["risks"].update(result["potential_risks"])

            # ì—”í‹°í‹° í†µí•©
            if "entities" in result:
                for entity_type, values in result["entities"].items():
                    if entity_type in summary["all_entities"]:
                        if isinstance(values, list):
                            summary["all_entities"][entity_type].update(values)

            # ì£¼ìš” ì •ë³´
            if "key_information" in result:
                if "emails" in result["key_information"]:
                    summary["all_entities"]["emails"].update(
                        result["key_information"]["emails"]
                    )
                if "social_media" in result["key_information"]:
                    summary["all_entities"]["social_media"].update(
                        result["key_information"]["social_media"]
                    )

            # í‚¤ì›Œë“œ
            if "keywords" in result:
                summary["top_keywords"].extend(result["keywords"][:3])

        # Setì„ Listë¡œ ë³€í™˜
        summary["all_entities"]["emails"] = list(summary["all_entities"]["emails"])[:10]
        summary["all_entities"]["urls"] = list(summary["all_entities"]["urls"])[:10]
        summary["all_entities"]["domains"] = list(summary["all_entities"]["domains"])[
            :10
        ]
        summary["risks"] = list(summary["risks"])

        # í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 10ê°œë§Œ
        from collections import Counter

        keyword_counts = Counter(summary["top_keywords"])
        summary["top_keywords"] = [word for word, _ in keyword_counts.most_common(10)]

        # ì£¼ìš” ë°œê²¬ì‚¬í•­
        if summary["all_entities"]["social_media"]:
            summary["key_findings"].append(
                f"Found social media links: {list(summary['all_entities']['social_media'].keys())}"
            )
        if summary["all_entities"]["emails"]:
            summary["key_findings"].append(
                f"Found {len(summary['all_entities']['emails'])} email addresses"
            )
        if len(results) > 1:
            summary["key_findings"].append(f"Crawled {len(results)} related pages")
        if summary["all_entities"]["urls"]:
            summary["key_findings"].append(
                f"Found {len(summary['all_entities']['urls'])} external URLs"
            )

        return summary


class VTClient:
    """VirusTotal ìœ„í˜‘ ì •ë³´ ì¡°íšŒ í´ë˜ìŠ¤"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.debug_mode = DEBUG_MODE

    def query_domain(self, domain: str) -> Dict[str, Any]:
        """ë„ë©”ì¸ í‰íŒ ì¡°íšŒ"""
        if self.debug_mode:
            return self._mock_domain_response(domain)

        if not self.api_key:
            return {
                "status": "error",
                "error": {"code": -32001, "message": "VirusTotal API KEY ì—†ìŒ"},
            }

        try:
            headers = {"x-apikey": self.api_key}
            url = f"https://www.virustotal.com/api/v3/domains/{domain}"

            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 404:
                return {
                    "status": "error",
                    "error": {
                        "code": -32003,
                        "message": f"ë„ë©”ì¸ '{domain}'ì„(ë¥¼) VirusTotal ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    },
                }
            elif response.status_code == 429:
                return {
                    "status": "error",
                    "error": {
                        "code": -32002,
                        "message": "VirusTotal ìš”ì²­ ì œí•œ ì´ˆê³¼ - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”",
                        "data": {"retry_after": 45},
                    },
                }
            elif response.status_code != 200:
                return {
                    "status": "error",
                    "error": {
                        "code": -32000,
                        "message": f"VirusTotal ì˜¤ë¥˜: {response.status_code}",
                    },
                }

            data = response.json()
            attributes = data.get("data", {}).get("attributes", {})

            last_analysis_stats = attributes.get(
                "last_analysis_stats",
                {"malicious": 0, "suspicious": 0, "undetected": 0},
            )

            threat_level = self._calculate_threat_level(last_analysis_stats)

            return {
                "status": "success",
                "data": {
                    "domain": domain,
                    "threat_level": threat_level,
                    "detected_by": last_analysis_stats.get("malicious", 0),
                    "analysis_stats": last_analysis_stats,
                    "last_analysis_date": attributes.get("last_analysis_date"),
                    "categories": attributes.get("categories", {}),
                },
                "metadata": {
                    "source": "virustotal",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "query_time_ms": 0,
                },
            }
        except requests.exceptions.Timeout:
            return {
                "status": "error",
                "error": {"code": -32005, "message": "VirusTotal ìš”ì²­ íƒ€ì„ì•„ì›ƒ"},
            }
        except Exception as e:
            return {
                "status": "error",
                "error": {"code": -32000, "message": f"VirusTotal ì˜¤ë¥˜: {str(e)}"},
            }

    def query_ip(self, ip_address: str) -> Dict[str, Any]:
        """IP ì£¼ì†Œ í‰íŒ ì¡°íšŒ"""
        if self.debug_mode:
            return self._mock_ip_response(ip_address)

        if not self.api_key:
            return {
                "status": "error",
                "error": {"code": -32001, "message": "VirusTotal API KEY ì—†ìŒ"},
            }

        try:
            headers = {"x-apikey": self.api_key}
            url = f"https://www.virustotal.com/api/v3/ip_addresses/{ip_address}"

            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 404:
                return {
                    "status": "error",
                    "error": {
                        "code": -32003,
                        "message": f"IP '{ip_address}'ì„(ë¥¼) VirusTotal ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    },
                }
            elif response.status_code == 429:
                return {
                    "status": "error",
                    "error": {
                        "code": -32002,
                        "message": "VirusTotal ìš”ì²­ ì œí•œ ì´ˆê³¼ - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”",
                        "data": {"retry_after": 45},
                    },
                }
            elif response.status_code != 200:
                return {
                    "status": "error",
                    "error": {
                        "code": -32000,
                        "message": f"VirusTotal ì˜¤ë¥˜: {response.status_code}",
                    },
                }

            data = response.json()
            attributes = data.get("data", {}).get("attributes", {})

            last_analysis_stats = attributes.get(
                "last_analysis_stats",
                {"malicious": 0, "suspicious": 0, "undetected": 0},
            )

            threat_level = self._calculate_threat_level(last_analysis_stats)

            return {
                "status": "success",
                "data": {
                    "ip_address": ip_address,
                    "country": attributes.get("country", "Unknown"),
                    "asn": attributes.get("asn"),
                    "organization": attributes.get("as_owner", "Unknown"),
                    "threat_level": threat_level,
                    "detected_by": last_analysis_stats.get("malicious", 0),
                    "analysis_stats": last_analysis_stats,
                },
                "metadata": {
                    "source": "virustotal",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "query_time_ms": 0,
                },
            }
        except requests.exceptions.Timeout:
            return {
                "status": "error",
                "error": {"code": -32005, "message": "VirusTotal ìš”ì²­ íƒ€ì„ì•„ì›ƒ"},
            }
        except Exception as e:
            logger.error(f"VirusTotal IP ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return {
                "status": "error",
                "error": {"code": -32000, "message": f"VirusTotal ì˜¤ë¥˜: {str(e)}"},
            }

    def _calculate_threat_level(self, stats: Dict[str, int]) -> str:
        """ìœ„í˜‘ ìˆ˜ì¤€ ê³„ì‚°"""
        malicious = stats.get("malicious", 0)
        suspicious = stats.get("suspicious", 0)

        if malicious >= 5:
            return "Critical"
        elif malicious >= 2:
            return "High"
        elif malicious + suspicious >= 5:
            return "Medium"
        elif malicious + suspicious > 0:
            return "Low"
        else:
            return "None"

    def _mock_domain_response(self, domain: str) -> Dict[str, Any]:
        """Mock ë„ë©”ì¸ ì‘ë‹µ"""
        return {
            "status": "success",
            "data": {
                "domain": domain,
                "threat_level": "Low",
                "detected_by": 0,
                "analysis_stats": {"malicious": 0, "suspicious": 0, "undetected": 89},
                "last_analysis_date": datetime.now(timezone.utc).isoformat(),
                "categories": {"Sophos": "legitimate", "Kaspersky": "legitimate"},
            },
            "metadata": {
                "source": "virustotal",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "query_time_ms": 245,
                "note": "Mock data in DEBUG_MODE",
            },
        }

    def _mock_ip_response(self, ip: str) -> Dict[str, Any]:
        """Mock IP ì‘ë‹µ"""
        return {
            "status": "success",
            "data": {
                "ip_address": ip,
                "country": "US",
                "asn": 15169,
                "organization": "Google LLC",
                "threat_level": "None",
                "detected_by": 0,
                "analysis_stats": {"malicious": 0, "suspicious": 0, "undetected": 89},
            },
            "metadata": {
                "source": "virustotal",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "query_time_ms": 198,
                "note": "Mock data in DEBUG_MODE",
            },
        }


# ============================================================================
# Phase 3: Client ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# ============================================================================

intelx_client = IntelligenceXClient(INTELX_API_KEY)
sherlock_client = SherlockClient()
playwright_client = PlaywrightClient()
vt_client = VTClient(VIRUSTOTAL_API_KEY)

# ============================================================================
# Phase 4: MCP ì„œë²„ ì´ˆê¸°í™”
# ============================================================================

server = FastMCP("osint-mcp-server", stateless_http=True)


# ============================================================================
# Phase 5: MCP Tools (@server.tool() ë°ì½”ë ˆì´í„° ì‚¬ìš©)
# ============================================================================


@server.tool()
def search_intelligence_x(request: SearchRequest) -> str:
    """
    Intelligence Xì—ì„œ ë‹¤í¬ì›¹ ë° ìœ ì¶œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        request: ê²€ìƒ‰ ìš”ì²­ ì •ë³´

    Returns:
        JSON í˜•ì‹ì˜ ê²€ìƒ‰ ê²°ê³¼
    """
    try:
        start_time = time.time()
        result = intelx_client.search(request)
        execution_time = (time.time() - start_time) * 1000

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"Intelligence X ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
def search_username_sherlock(request: SherlockSearchRequest) -> str:
    """
    Sherlockì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìëª…ì„ ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        request: Sherlock ê²€ìƒ‰ ìš”ì²­ ì •ë³´

    Returns:
        JSON í˜•ì‹ì˜ ê²€ìƒ‰ ê²°ê³¼
    """
    try:
        start_time = time.time()
        result = sherlock_client.search(request)
        execution_time = (time.time() - start_time) * 1000

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"Sherlock ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
async def analyze_url_playwright(request: PlaywrightAnalyzeRequest) -> str:
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ URLì„ ë¶„ì„í•©ë‹ˆë‹¤.

    Args:
        request: Playwright ë¶„ì„ ìš”ì²­ ì •ë³´

    Returns:
        JSON í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼
    """
    try:
        start_time = time.time()
        result = await playwright_client.analyze(request)
        execution_time = (time.time() - start_time) * 1000

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"URL ë¶„ì„ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
def check_virustotal_domain(request: ThreatIntelRequest) -> str:
    """
    VirusTotalì—ì„œ ë„ë©”ì¸ì˜ ìœ„í˜‘ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

    Args:
        request: ìœ„í˜‘ ì •ë³´ ì¡°íšŒ ìš”ì²­ (query_type: 'domain' ì‚¬ìš©)

    Returns:
        JSON í˜•ì‹ì˜ ìœ„í˜‘ ì •ë³´
    """
    try:
        start_time = time.time()
        result = vt_client.query_domain(request.query)
        execution_time = (time.time() - start_time) * 1000

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"VirusTotal ë„ë©”ì¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
def check_virustotal_ip(request: ThreatIntelRequest) -> str:
    """
    VirusTotalì—ì„œ IP ì£¼ì†Œì˜ ìœ„í˜‘ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

    Args:
        request: ìœ„í˜‘ ì •ë³´ ì¡°íšŒ ìš”ì²­ (query_type: 'ip' ì‚¬ìš©)

    Returns:
        JSON í˜•ì‹ì˜ ìœ„í˜‘ ì •ë³´
    """
    try:
        start_time = time.time()
        result = vt_client.query_ip(request.query)
        execution_time = (time.time() - start_time) * 1000

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"VirusTotal IP ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@server.tool()
async def crawl_and_analyze_url(request: PlaywrightCrawlRequest) -> str:
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ URLì„ ìë™ìœ¼ë¡œ í¬ë¡¤ë§í•˜ê³  ì§€ëŠ¥í˜• ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        request: Playwright í¬ë¡¤ë§ ìš”ì²­ ì •ë³´

    Returns:
        JSON í˜•ì‹ì˜ í¬ë¡¤ë§ ê²°ê³¼
    """
    try:
        start_time = time.time()
        result = await playwright_client.crawl(request)
        execution_time = (time.time() - start_time) * 1000

        return json.dumps(
            {
                **result,
                "execution_time_ms": int(execution_time),
            },
            indent=2,
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


if __name__ == "__main__":
    logger.info("=" * 70)
    logger.info("Starting OSINT Unified MCP Server (fastmcp - STDIO mode)")
    logger.info("=" * 70)
    logger.info(f"DEBUG ëª¨ë“œ: {DEBUG_MODE}")
    logger.info("Transport: STDIO (Claude Desktop í˜¸í™˜)")
    logger.info("")
    logger.info("âœ… í™œì„±í™”ëœ OSINT ë„êµ¬:")
    logger.info("   1. search_intelligence_x - ë‹¤í¬ì›¹/ìœ ì¶œ ë°ì´í„° ê²€ìƒ‰")
    logger.info("   2. search_username_sherlock - ì‚¬ìš©ìëª… ê²€ìƒ‰")
    logger.info("   3. analyze_url_playwright - URL ë¶„ì„")
    logger.info("   4. check_virustotal_domain - VirusTotal ë„ë©”ì¸ í™•ì¸")
    logger.info("   5. check_virustotal_ip - VirusTotal IP í™•ì¸")
    logger.info("   6. crawl_and_analyze_url - URL ìë™ í¬ë¡¤ë§ & ì§€ëŠ¥í˜• ë¶„ì„")
    logger.info("=" * 70)

    server.run()
