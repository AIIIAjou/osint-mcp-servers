"""
í–¥ìƒëœ OSINT ì›¹ ì¸í„°í˜ì´ìŠ¤
- í’ë¶€í•œ ë°ì´í„° ì‹œê°í™”
- Ollama LLM ì±„íŒ… í†µí•©
"""

import os
import json
from typing import Optional, List, Dict, Any
from datetime import datetime
from fastapi import FastAPI, Query, HTTPException, Request
from fastapi.responses import HTMLResponse, FileResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import requests
import asyncio

from db_manager import OSINTDatabase


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Enhanced OSINT Dashboard",
    description="OSINT ìˆ˜ì§‘ ì •ë³´ ëŒ€ì‹œë³´ë“œ + LLM ì±„íŒ…",
    version="2.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
db = OSINTDatabase("db.csv")

# Ollama ì„¤ì •
OLLAMA_API_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen3:8b")


class ChatMessage(BaseModel):
    message: str
    model: Optional[str] = None


@app.get("/", response_class=HTMLResponse)
async def root():
    """ë©”ì¸ ëŒ€ì‹œë³´ë“œ í˜ì´ì§€"""
    with open("web_dashboard.html", "r", encoding="utf-8") as f:
        return f.read()


@app.get("/api/statistics")
async def get_statistics():
    """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´ ë°˜í™˜"""
    return db.get_statistics()


@app.get("/api/records")
async def get_records(
    target: Optional[str] = None,
    collection_method: Optional[str] = None,
    threat_level: Optional[str] = None
):
    """ëª¨ë“  ë ˆì½”ë“œ ë˜ëŠ” í•„í„°ë§ëœ ë ˆì½”ë“œ ë°˜í™˜"""
    if target or collection_method or threat_level:
        return db.search_records(
            target=target,
            collection_method=collection_method,
            threat_level=threat_level
        )
    return db.get_all_records()


@app.delete("/api/records/{timestamp}")
async def delete_record(timestamp: str):
    """íŠ¹ì • ë ˆì½”ë“œ ì‚­ì œ"""
    success = db.delete_record(timestamp)
    if not success:
        raise HTTPException(status_code=404, detail="ë ˆì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return {"message": "ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}


@app.get("/api/pdf")
async def get_pdf(path: str):
    """PDF íŒŒì¼ ìƒˆ íƒ­ì—ì„œ ì—´ê¸°"""
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return FileResponse(
        path,
        media_type="application/pdf",
        headers={"Content-Disposition": "inline"}
    )


@app.get("/api/export")
async def export_database():
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    output_path = "db_export.json"
    success = db.export_to_json(output_path)
    if not success:
        raise HTTPException(status_code=500, detail="ë‚´ë³´ë‚´ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    return FileResponse(
        output_path,
        media_type="application/json",
        filename=output_path
    )


@app.post("/api/chat")
async def chat_with_llm(message: ChatMessage):
    """
    Ollama LLMê³¼ ì±„íŒ…

    Args:
        message: ì‚¬ìš©ì ë©”ì‹œì§€

    Returns:
        LLM ì‘ë‹µ
    """
    try:
        # Ollama API í˜¸ì¶œ (ì˜¬ë°”ë¥¸ ì—”ë“œí¬ì¸íŠ¸)
        response = requests.post(
            f"{OLLAMA_API_URL}/api/chat",
            json={
                "model": message.model or OLLAMA_MODEL,
                "messages": [
                    {
                        "role": "user",
                        "content": message.message
                    }
                ],
                "stream": False
            },
            timeout=60
        )

        if response.status_code == 200:
            result = response.json()
            # Ollama API ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
            assistant_message = result.get("message", {})
            return {
                "success": True,
                "response": assistant_message.get("content", ""),
                "model": message.model or OLLAMA_MODEL
            }
        else:
            # ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€
            error_detail = f"Ollama API ì˜¤ë¥˜ (HTTP {response.status_code})"
            try:
                error_json = response.json()
                error_detail += f": {error_json.get('error', response.text)}"
            except:
                error_detail += f": {response.text[:200]}"

            return {
                "success": False,
                "error": error_detail
            }

    except requests.exceptions.ConnectionError:
        return {
            "success": False,
            "error": "Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ollama serve' ëª…ë ¹ìœ¼ë¡œ Ollamaë¥¼ ì‹œì‘í•˜ì„¸ìš”."
        }
    except requests.exceptions.Timeout:
        return {
            "success": False,
            "error": "Ollama ì‘ë‹µ ì‹œê°„ ì´ˆê³¼. ëª¨ë¸ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì„œë²„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"
        }


@app.post("/api/chat/stream")
async def chat_with_llm_stream(message: ChatMessage):
    """
    Ollama LLMê³¼ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…

    Args:
        message: ì‚¬ìš©ì ë©”ì‹œì§€

    Returns:
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
    """
    async def generate():
        try:
            response = requests.post(
                f"{OLLAMA_API_URL}/api/chat",
                json={
                    "model": message.model or OLLAMA_MODEL,
                    "messages": [
                        {
                            "role": "user",
                            "content": message.message
                        }
                    ],
                    "stream": True
                },
                stream=True,
                timeout=120
            )

            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    if "message" in data:
                        content = data["message"].get("content", "")
                        if content:
                            yield f"data: {json.dumps({'text': content})}\n\n"
                    if data.get("done"):
                        yield f"data: {json.dumps({'done': True})}\n\n"
                        break

        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")


@app.get("/api/ollama/models")
async def get_ollama_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            return {
                "success": True,
                "models": [m["name"] for m in models]
            }
        else:
            return {
                "success": False,
                "error": "ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


@app.get("/api/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ ì²´í¬"""
    # Ollama ì—°ê²° í™•ì¸
    ollama_status = "disconnected"
    try:
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=2)
        if response.status_code == 200:
            ollama_status = "connected"
    except:
        pass

    return {
        "status": "healthy",
        "ollama": ollama_status,
        "database": "connected",
        "records": db.get_statistics()["total_records"]
    }


if __name__ == "__main__":
    print("=" * 70)
    print("ğŸŒ Enhanced OSINT Dashboard ì‹œì‘")
    print("=" * 70)
    print("ğŸ“Š ëŒ€ì‹œë³´ë“œ: http://localhost:8000")
    print("ğŸ“– API ë¬¸ì„œ: http://localhost:8000/docs")
    print("ğŸ’¬ LLM ì±„íŒ…: ì›¹ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥")
    print()
    print("âš™ï¸  ì„¤ì •:")
    print(f"  - Ollama URL: {OLLAMA_API_URL}")
    print(f"  - ê¸°ë³¸ ëª¨ë¸: {OLLAMA_MODEL}")
    print("=" * 70)

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
